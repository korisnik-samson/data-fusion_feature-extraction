{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPHda098DdnJ"
   },
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tz-u4zK6DdnM",
    "ExecuteTime": {
     "end_time": "2025-02-18T22:36:54.520078Z",
     "start_time": "2025-02-18T22:36:52.912648Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#target = -1 + 2*x1 + 2*x2 #np.array([2,2,-1])\n",
    "df = pd.DataFrame({\n",
    "    'x1': [0, 1, 0, -0.5, -1, 2, 1, 3, 0.5, -1, 0, -2, -3, 1, 2, -4, -3, 3, -0.5, -2],\n",
    "    'x2': [0, 1, 1, 0, -1, 4, -1, -5, -2, 1, 3, 0, -1, 6, 8, 2, 3, 10, 2, 1],\n",
    "    'y': [1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "})\n",
    "\n",
    "df['x0'] = 1\n",
    "df = df[['x0', 'x1', 'x2', 'y']]\n",
    "\n",
    "\n",
    "def perceptron(points, dim, max_it=100, use_adaline=False, eta=1, randomize=False, print_out=True):\n",
    "    w = np.zeros(dim + 1)\n",
    "    xs, ys = points[:, :dim + 1], points[:, dim + 1]\n",
    "    num_points = points.shape[0]\n",
    "    \n",
    "    for it in range(max_it):\n",
    "        correctly_predicted_ids = set()\n",
    "        idxs = np.arange(num_points)\n",
    "        \n",
    "        if randomize:\n",
    "            idxs = np.random.choice(np.arange(num_points), num_points, replace=False)\n",
    "            \n",
    "        for idx in idxs:\n",
    "            x, y = xs[idx], ys[idx]\n",
    "            st = np.dot(w.T, x)\n",
    "            prod = st * y  #np.dot(w.T, x)*y\n",
    "            \n",
    "            if prod < -100:  #avoid out of bound error\n",
    "                st = -100\n",
    "                \n",
    "            threshold = 1 if use_adaline else 0\n",
    "            st = st if use_adaline else 0\n",
    "            \n",
    "            if prod <= threshold:\n",
    "                w = w + eta * (y - st) * x\n",
    "                break  #PLA picks one example at each iteration\n",
    "            else:\n",
    "                correctly_predicted_ids.add(idx)\n",
    "                \n",
    "        if len(correctly_predicted_ids) == num_points:\n",
    "            break\n",
    "\n",
    "    c = 0\n",
    "    \n",
    "    for x, y in zip(xs, ys):\n",
    "        prod = np.dot(w.T, x) * y\n",
    "        if prod > 0:\n",
    "            c += 1\n",
    "            \n",
    "    w = w / w[-1]\n",
    "    \n",
    "    if print_out:\n",
    "        print('final correctness: ', c, '. Total iteration: ', it)\n",
    "        print('final normalized w:', w)\n",
    "        \n",
    "    return w, it\n",
    "\n",
    "\n",
    "def flip_coins(total_coins):\n",
    "    \"\"\"Flip all coins once, return their head/tail status\n",
    "    \"\"\"\n",
    "\n",
    "    hts = np.zeros(total_coins)  #head: 1, tail: 0\n",
    "    probs = np.random.uniform(size=total_coins)\n",
    "    hts[probs > 0.5] = 1\n",
    "    return hts\n",
    "\n",
    "\n",
    "def run_once(total_coins, total_flips, print_freq=False):\n",
    "    v1, vrand, vmin = None, None, None\n",
    "    crand = np.random.choice(total_coins)\n",
    "    hts_sum = np.zeros(total_coins)  # store the sum of heads in total_flips\n",
    "\n",
    "    for flip in range(total_flips):\n",
    "        hts_sum = hts_sum + flip_coins(total_coins)\n",
    "\n",
    "    hts_freq = hts_sum / total_flips\n",
    "\n",
    "    v1 = hts_freq[0]\n",
    "    vrand = hts_freq[crand]\n",
    "    cmin = np.argmin(hts_sum)\n",
    "    vmin = hts_freq[cmin]\n",
    "\n",
    "    if print_freq:\n",
    "        print('Frequency of first coin: {}'.format(v1))\n",
    "        print('Frequency of a random coin: id({})-freq({})'.format(crand, vrand))\n",
    "        print('Frequency of the coin with minimum frequency: id({})-freq({})'.format(cmin, vmin))\n",
    "        \n",
    "    return v1, vrand, vmin\n",
    "\n",
    "\n",
    "def hoeffding_bound(epsilon, n):\n",
    "    return 2.0 * np.exp(-2.0 * n * epsilon ** 2)\n",
    "\n",
    "\n",
    "def generate_random_numbers01(N, dim, num_grid_points):\n",
    "    random_ints = np.random.randint(num_grid_points, size=(N, dim))\n",
    "    init_lb = 0\n",
    "    \n",
    "    return (random_ints - init_lb) / (num_grid_points - 1 - init_lb)\n",
    "\n",
    "\n",
    "def generate_random_numbers(N, dim, num_grid_points, lb, ub):\n",
    "    zero_to_one_points = generate_random_numbers01(N, dim, num_grid_points)\n",
    "    res = lb + (ub - lb) * zero_to_one_points\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_random_coeffs(dim):\n",
    "    rn = generate_random_numbers(1, dim, 1000, -10, 10)\n",
    "    \n",
    "    return rn\n",
    "\n",
    "\n",
    "def true_f(x, coeffs):\n",
    "    return coeffs.flatten()[0] + np.dot(coeffs.flatten()[1:], x.flatten())\n",
    "\n",
    "\n",
    "def generate_two_classes(N, dim, true_func, rn_func):\n",
    "    cls1, cls2 = [], []\n",
    "    \n",
    "    while True:\n",
    "        rn = rn_func(1, dim).flatten()\n",
    "        if true_func(rn) > 0 and len(cls1) < N:\n",
    "            cls1.append(rn)\n",
    "        elif true_func(rn) < 0 and len(cls2) < N:\n",
    "            cls2.append(rn)\n",
    "        if len(cls1) == N and len(cls2) == N:\n",
    "            break\n",
    "            \n",
    "    return np.asarray(cls1), np.asarray(cls2)\n",
    "\n",
    "\n",
    "def generate_df(N, dim, true_func, rn_func):\n",
    "    cls1, cls2 = generate_two_classes(N / 2, dim, true_func, rn_func)\n",
    "    cols = ['x' + str(i) for i in range(1, dim + 1)]\n",
    "    \n",
    "    df1 = pd.DataFrame(cls1, columns=cols)\n",
    "    df1['y'] = 1\n",
    "    \n",
    "    df2 = pd.DataFrame(cls2, columns=cols)\n",
    "    df2['y'] = -1\n",
    "    \n",
    "    df = pd.concat([df1, df2])\n",
    "    df['x0'] = 1\n",
    "    \n",
    "    df = df[['x0'] + cols + ['y']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_data(x1, df, norm_coeffs, norm_g, lb, ub):\n",
    "    figsize = plt.figaspect(1)\n",
    "    f, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    cls1_df = df.loc[df['y'] == 1]\n",
    "    cls2_df = df.loc[df['y'] == -1]\n",
    "    \n",
    "    line = ax.plot(x1, -(norm_coeffs[0] + norm_coeffs[1] * x1), label='True Function')\n",
    "    pluses = ax.scatter(cls1_df[['x1']].values, cls1_df[['x2']].values, marker='+', c='r', label='+1 labels')\n",
    "    minuses = ax.scatter(cls2_df[['x1']].values, cls2_df[['x2']].values, marker=r'$-$', c='b', label='-1 labels')\n",
    "    \n",
    "    if norm_g is not None:\n",
    "        hypothesis = ax.plot(x1, -(norm_g[0] + norm_g[1] * x1), c='r', label='Final Hypothesis')\n",
    "\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=11)\n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=11)\n",
    "    ax.set_title('Data set size = %s' % N, fontsize=9)\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    legend_x = 2.0\n",
    "    legend_y = 0.5\n",
    "    \n",
    "    ax.legend(['True Function', '+1 labels', '-1 labels', 'Final Hypothesis', ],\n",
    "              loc='center right', bbox_to_anchor=(legend_x, legend_y))\n",
    "    #ax.legend(handles=[pluses, minuses], fontsize=9)\n",
    "    \n",
    "    ax.set_ylim(bottom=lb, top=ub)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_perceptron_experiment(N, dim, lb, ub, num_grid_points, coeff_lb, coeff_ub,\n",
    "                              eta, maxit, use_adaline=False, randomize=False, show_plot=True, test_N=10000):\n",
    "    \n",
    "    rns = generate_random_numbers(N, dim, num_grid_points, lb, ub)\n",
    "    rn_func = partial(generate_random_numbers, num_grid_points=num_grid_points, lb=lb, ub=ub)\n",
    "    \n",
    "    coeffs = generate_random_numbers(1, dim + 1, num_grid_points, coeff_lb, coeff_ub)\n",
    "    norm_coeffs = coeffs.flatten() / coeffs.flatten()[-1]\n",
    "    true_func = partial(true_f, coeffs=norm_coeffs)\n",
    "\n",
    "    df = generate_df(N, dim, true_func, rn_func)\n",
    "    test_df = generate_df(test_N, dim, true_func, rn_func)\n",
    "\n",
    "    x1 = np.arange(lb, ub, 0.01)\n",
    "    norm_g, num_its = perceptron(df.values, dim, maxit, use_adaline,\n",
    "                                 eta, randomize, show_plot)\n",
    "\n",
    "    if show_plot:\n",
    "        print('True coeffs: ', norm_coeffs)\n",
    "        plot_data(x1, df, norm_coeffs, norm_g, lb, ub)\n",
    "        \n",
    "    return num_its, norm_g, test_df"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-WNEY1BDdnP"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "#### Exercise 1.1\n",
    "Express each of the following tasks in the framework of learning from data by specifying the input space $X$, output space $Y$, target function $f$: $X$ -> $Y$, and the specifics of the data set that we will learn from.\n",
    "1. (a)  Medical diagnosis: A patient walks in with a medical history and some symptoms, and you want to identify the problem.\n",
    "  * Input space $\\mathcal{X}$: patient's medical history, symptoms, personal health information etc.\n",
    "  * Output space $\\mathcal{Y}$: all possible diseases\n",
    "  * target function $f: \\mathcal{X} \\to \\mathcal{Y}$: ideal formula to identify a patient's problem\n",
    "  * Data set: All available patients' information and their corresponding correct problem diagnostic.\n",
    "2. (b) Handwritten digit recognition (for example postal zip code recognition for mail sorting).\n",
    "  * Input space $\\mathcal{X}$: handwritten digits (digitalized)\n",
    "  * Output space $\\mathcal{Y}$: 0-9 digits\n",
    "  * target function $f: \\mathcal{X} \\to \\mathcal{Y}$: ideal formula match a handwritten digit to a correct digit\n",
    "  * Data set: handwritten digits and their corresponding correct matches\n",
    "3. (c) Determining if an email is spam or not.\n",
    "  * Input space $\\mathcal{X}$: every information of an email, e.g. words\n",
    "  * Output space $\\mathcal{Y}$: yes/no\n",
    "  * target function $f: \\mathcal{X} \\to \\mathcal{Y}$: ideal formula to identify whether an email is spam or not\n",
    "  * Data set: Spam and non-spam emails that have been identified by human\n",
    "4. (d) Predicting how an electric load varies with price, temperature, and day of the week.\n",
    "  * Input space $\\mathcal{X}$: price of electric, temperature, day of the week\n",
    "  * Output space $\\mathcal{Y}$: electric load\n",
    "  * target function $f: \\mathcal{X} \\to \\mathcal{Y}$: ideal function that gives exact electric load for a given price, temperature and day of the week.\n",
    "  * Data set: historical electric load along with corresponding price, temperature and day of the week information.\n",
    "5. (e) A problem of interest to you for which there is no analytic solution, but you have data from which to construct an empirical solution.\n",
    "  * Input space $\\mathcal{X}$: dog images\n",
    "  * Output space $\\mathcal{Y}$: types of dogs\n",
    "  * target function $f: \\mathcal{X} \\to \\mathcal{Y}$: ideal function that gives dog type according to the picture\n",
    "  * Data set: picutres with dogs that have been categorized to various types\n",
    "\n",
    "#### Exercise 1.2\n",
    "Suppose that we use a perceptron to detect spam messages. Let's say that each email message is represented by the frequency of occurrence of keywords, and the output is if the message is considered spam.\n",
    "\n",
    "(a) Can you think of some keywords that will end up with a large positive weight in the perceptron?\n",
    "\n",
    "(b) How about keywords that will get a negative weight?\n",
    "\n",
    "(c) What parameter in the perceptron directly affects how many borderline messages end up being classified as spam?\n",
    "\n",
    "1. (a) Keywords with a large positive weight: free, cheap, earn\n",
    "\n",
    "1. (b) Keywords with a negative weight: person name, hi, the\n",
    "\n",
    "1. (c) The parameter $b$ in perceptron directly affects how many borderline messages end up being classified as spam. This is because $b$ is the threshold used to classify the emails into spam and non-spam categories.\n",
    "\n",
    "#### Exercise 1.3\n",
    "The weight update rule ($w(t+1)= w(t)+y(t)x(t)$) has the nice interpretation that it moves in the direction of classifying $x(t)$ correctly.\n",
    "\n",
    "(a) Show that $y(t)w^T(t)x(t) < 0$. [Hint: $x(t)$ is misclassified by $w(t)$.]\n",
    "\n",
    "(b) Show that $y(t)w^T(t+l)x(t) > y(t)w^T(t)x(t)$. [Hint: Use $w(t+1)= w(t)+y(t)x(t).$]\n",
    "\n",
    "(c) As far as classifying $x(t)$ is concerned, argue that the move from $w(t)$ to $w(t+1)$ is a move 'in the right direction'.\n",
    "\n",
    "\n",
    "1. (a) If $x(t)$ is misclassified by $w(t)$, then $w^T(t)x(t)$ has different signs of $y(t)$, thus $y(t)w^T(t)x(t) \\lt 0$.\n",
    "\n",
    "1. (b) \\begin{align*}\n",
    "y(t)w^T(t+1)x(t) &= y(t) \\left(w(t)+y(t)x(t)\\right)^Tx(t) \\\\\n",
    "&= y(t)\\left(w^T(t) + y(t)x^T(t)\\right)x(t) \\\\\n",
    "&= y(t)w^T(t)x(t) + y(t)y(t)x^T(t)x(t)\\\\\n",
    "&\\gt y(t)w^T(t)x(t) \\;\\;\\;\\text{because the last term is } \\ge \\text{ than } 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "1. (c) From previous problem, we see that $y(t)w^T(t)x(t)$ is increasing with each update.\n",
    "\n",
    "If $y(t)$ is positive, but $w^T(t)x(t)$ is negative, we move $w^T(t)x(t)$ toward positive by increasing it.\n",
    "\n",
    "If however $y(t)$ is negative, but $w^T(t)x(t)$ is positive, $y(t)w^T(t)x(t)$ increases means $w^T(t)x(t)$ is decreasing, i.e. moving toward negative region.\n",
    "\n",
    "So the move from $w(t)$ to $w(t+1)$ is a move \"in the right direction\" as far as classifying $x(t)$ is concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8ipTxQiDdnQ"
   },
   "source": [
    "#### Exercise 1.4\n",
    "Let us create our own target function $f$ and data set $D$ and see how the perceptron learning algorithm works. Take $d$ = 2 so you can visualize the problem, and choose a random line in the plane as your target function, where one side of the line maps to 1 and the other maps to -1. Choose the inputs $x_{n}$ of the data set as random points in the plane, and evaluate the target function on each $x_{n}$ to get the corresponding output $y_{n}$.\n",
    "\n",
    "Now, generate a data set of size 20. Try the perceptron learning algorithm on your data set and see how long it takes to converge and how well the final hypothesis $g$ matches your target $f$.\n",
    "\n",
    "\n",
    "Implement the perceptron learning algorithm and check:\n",
    "* Convergence speed: The convergence is fast, it depends on the data, but usually it only takes about 10 iterations to find a solution.\n",
    "* How well the final hypothesis $g$ matches your target $f$: The final hypothesis $g$ doesn't match my target $f$ very closely in terms of their coefficients. But from picture, they are largely in line with each other in the range of data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "2PVTLDXgDdnQ",
    "outputId": "dfb5752c-ca6a-4c66-803a-a8bbb2ab2b63",
    "ExecuteTime": {
     "end_time": "2025-02-18T22:37:03.409326Z",
     "start_time": "2025-02-18T22:37:03.142087Z"
    }
   },
   "source": [
    "#perceptron(df.values, 2)\n",
    "\n",
    "lb, ub = -100, 100\n",
    "N, dim = 20, 2\n",
    "num_grid_points = 2000\n",
    "coeff_lb, coeff_ub = -10, 10\n",
    "eta = 1.0\n",
    "maxit = 100\n",
    "use_adaline, randomize = False, False\n",
    "_, _, _ = run_perceptron_experiment(N, dim, lb, ub, num_grid_points, coeff_lb, coeff_ub, eta, maxit, use_adaline, randomize)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final correctness:  20 . Total iteration:  10\n",
      "final normalized w: [ 0.0160048  -0.19855885  1.        ]\n",
      "True coeffs:  [ 0.61107065 -0.10852149  1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAHECAYAAADxtl/0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUx1JREFUeJzt3Qd4lFX2x/GTkAakUVOkSwfpiChSFMW6i6JrpaiASFEEpawFEFdEV+yi/lHQtSGKqGBDEBUFBKQIQuhFOgIJgRRC5v+cG95hJpNOkpl58/08z7uTmbmZeTNmh/nl3HtugMPhcAgAAAAA2Eigt08AAAAAAIobQQcAAACA7RB0AAAAANgOQQcAAACA7RB0AAAAANgOQQcAAACA7RB0AAAAANgOQQcAAACA7RB0AAAAANgOQQdAkfz8889So0YNb58GAABAjgg6wDno2rWrhIaGSkREhERFRUnz5s1l5MiRcujQoQI/xvjx46Vnz57iDf369ZPhw4cX6XsvvfRS+euvv8RXpKWlyYABA6Ru3brmv0fjxo3l7bffdhuTlJQkt99+u0RGRkpMTIxMnDjRa+cLAABKFkEHOEeTJ0+W48ePy7Fjx+Tjjz+WPXv2SNu2beXAgQPePrUyJSMjQ+Li4uT77783gWbGjBkmdH733XfOMcOGDZMjR47Irl27TEXq//7v/+Tdd9/16nkDAICSQdABiklAQIA0bdpU3nvvPVMxeO6558ztycnJ8s9//lOqV69uqj6dO3eWNWvWmPvmzJkjTz31lMydO1fCw8PNofTDebt27cx4/fA+ePBgSUlJyfW5p0yZIrVq1TKVjDp16si0adOc9+kH/wsvvFCio6OlWbNm8sUXX5jbX3rpJXn//ffltddeM8+r9+VExzRo0MA89nnnneesgixatMg8plq+fLnz/PWoWLGieT127Nhh7v/999+lW7duUrlyZalfv74JGMVNn/OJJ56Q888/3zz3RRddZJ5z8eLF5v6TJ0/KRx99JE8++aQ574YNG5rg89ZbbxX7uQAAAO8j6ADFLCgoyExF+/HHH831zMxMM11q+/btpsrTunVr+de//iUOh8OM+/e//y3XXXedCUR6qPLly5swoNWHX375RX744QcTZnKyadMmefTRR0040srSsmXLTLBRa9eulZtvvlmefvpp81hvvPGG9O7dWxISEuT++++XO+64w4Qofd7169d7PPaJEyfM9DYNA/rYOuaqq67yGNe+fXvn+euhP6+GDF3Ds3//frniiivkvvvuM1P6NNyNGzdOFixYkOPPo8FEg0huh55vQaSmpspvv/0mLVq0MNf1Z05PT5dWrVo5x+jX+hoBAAD7IegAJUArHxoslFZ3brnlFlNxCAsLkwkTJphwsnfv3jzXv2ggKleunNSrV0/uvfdeU0HJiY7R0KQhRKs+uvbE+nCvwUaDymWXXSaBgYHSqVMnE6p0il1BBQcHy4YNG8x0MA0aGmry8swzz5hpYbNnzzah73//+5+pYmm403PVdUx33XWXfPDBBzl+v56jTgPM7dAKVH709ejfv7+pRN14443mNg1g+t9Az8miP48GOAAAYD8EHaAE6DodnaalNHxoFUKnlGno0Ut1+PDhXL9fp4J1797dhBb9Hq365DZep2q988478sorr5jxV155paxevdrcp1PHXn/9dbeKyOeff55nyHKlweDLL78031OzZk0TQrS6lJvPPvvMTNmbN2+ec1qbnsNXX33ldg46bW7fvn1SEjTk6OutFRytHmnAUzqlTqev6VoeS2JiopmSBwAA7IegAxQz/SCtwUA7sin94L9y5UozJUurIta6Ff1ArqwP4q5uu+02M/Vr27Zt5nt0HY81PidaLdEAolPjWrZsaaanKQ0nDzzwgFtFRCsbU6dOzfW5s7v88stNUNGgpdPgdLqdTsfLbsWKFXLPPffIp59+asKXRc/hhhtucDsHraLoY+ZEq0Gu632yH4MGDcr1XPU1GjJkiJm+p1P5dI2TpVGjRqY6Za2PUhoIL7jggnxfAwAA4H8IOkAx2rhxo/Tt29dUCkaMGGFu06CiU9YqVapkQoZWZ1xpFWbnzp1ulQZrmphWVHTamBVMcqKVi/nz55vKUUhIiAkD1vQsnfI2ffp0E4JOnz5tWjAvWbLEPKb13BqmcgtRGpy0SqPBRB9Tq0uuU78s2mZaGy68/PLLpurjSkPXwoULTQA6deqUOTRgaNUqt2l7rut9sh9aocrN0KFDzZomfT309XZVoUIFM4XwscceM/99Nm/ebM5Xp7gBAAD7IegA52j06NHOfXR0PUhsbKypbmiIUBp4dG2KXtf1KR07dnT7fq2SaICoVq2ac7qXrq3573//66xg3Hrrrbk+vy6w1w/v+vhVqlQxoUJbKytd5/Phhx+aZgX6+Lp2SMdq4FH6Id+aZmet63GllZsXX3zRVGX053v11Vflk08+8agEaWc3nYqmwcq1+qJtnPU5v/32W/MzaQc5PU+tumiYK04aFnX9jga/2rVr51gB0ul9+nNok4RLLrnEVKD69OlTrOcBAAB8Q4Ajr/kwAAAAAOCHqOgAAAAAsB2CDgAAAADbIegAAAAAsB2/Czo//fSTXH/99RIfHy8BAQFmnwxXuuTo8ccfN4uedXd53YtEuyu50o0cdUd4XQCui791QbK1Iz0AAAAA/+d3QefEiRNmnxDt/pTbruy6GaG2oNW9NLQ9b48ePSQ1NdU5RkOO7iKvLWjnzp1rwtPAgQNL8acAAAAAUJL8uuuaVnR0jw/dwFDpj6KVnpEjR8pDDz1kbtP9MrSdrbbb1Ra9un9I06ZNzR4e7dq1M2O++eYbueaaa8xeIPr9OdF2vFZLXqvtrlaGtJ2vngcAwD/pvx26V5S+/xdkE10AgH/w3PnPj23fvl32799vpqtZdM+MDh06mE0SNejopU5Xs0KO0vH6j5tWgHQH95xMmjRJJkyYUCo/BwCg9O3evdvssQQAsAdbBR0NOcraqNGi16379LJ69epu9+tO77phojUmJ2PHjnXudG9VimrVqmX+YdS1PgAA/6Sb1+qmuLrxLwDAPmwVdEpSaGioObLTkEPQAQD/xzRkALAXW01Gjo2NNZcHDhxwu12vW/fp5cGDB93uz8jIMOttrDEAAAAA/Jutgk7dunVNWFmwYIHblARde9OxY0dzXS+PHTsmK1eudI5ZuHChaS6ga3kAAAAA+D+/m7qm+91s2bLFrQHB6tWrzRobXTMzfPhwefLJJ6VBgwYm+Dz22GOmk47Vma1JkyZy1VVXyYABA0wL6lOnTsnQoUNNo4LcOq4BAAAA8C9+F3RWrFgh3bp1c163GgT07dvXtJAeNWqU2WtH98XRyk2nTp1M++iwsDDn97z//vsm3Fx++eWm21qvXr3M3jsAAAAA7MGv99HxJp0Sp62rtfsazQgAwH/xfg4A9mSrNToAAAAAoAg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6AAAAAGyHoAMAAADAdgg6yNuJEyIBAVmHfg0AAAD4AYIOAAAAANsJ8vYJwEdZ1RvXKo7r1xUrlv45AQAAAAVE0EHOwsM9b4uJOfu1w1GqpwMAAAAUBlPXAAAAANgOQQc5S07OOg4cOHubfm3dXlpohgAAAIAiYOoacpbTGhy9jbU5AAAA8AMEHfgmmiEAAADgHBB0kDcNFN5oPEAzBAAAAJwD1ugAAAAAsB0qOvBNVsMDna5mVXK0GQJT1gAAAFAABB34JpohAAAA4BwwdQ0AAACA7VDRgW/zVjMEAAAA+DUqOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHYIOgAAAABsh6ADAAAAwHZsGXTq1KkjAQEBHseQIUPM/V27dvW4b9CgQd4+bQAAAADFJEhsaPny5XL69Gnn9XXr1skVV1whN998s/O2AQMGyBNPPOG8XqFChVI/TwAAAAAlw5ZBp1q1am7Xn376aTn//POlS5cubsEmNjbWC2cHAAAAoKTZcuqaq/T0dHnvvffk7rvvNlPULO+//75UrVpVmjdvLmPHjpWTJ0/m+ThpaWmSlJTkdgCAXzhxQkTf//TQrwEAKANsWdFxNWfOHDl27Jj069fPedvtt98utWvXlvj4eFm7dq2MHj1aEhISZPbs2bk+zqRJk2TChAmldNYAAAAAzkWAw+FwiI316NFDQkJC5Msvv8x1zMKFC+Xyyy+XLVu2mCluuVV09LBoRadmzZqSmJgokZGRJXLuAHBOrOqNXsbEZH194IBIxYpZX1uXZZy+n0dFRfF+DgA2Y+uKzs6dO+X777/Ps1KjOnToYC7zCjqhoaHmAAC/ER7ueZsVeJS9/84FACjjbL1GZ/r06VK9enW59tpr8xy3evVqcxkXF1dKZwYAAACgJNm2opOZmWmCTt++fSUo6OyPuXXrVvnggw/kmmuukSpVqpg1Og8++KB07txZWrRo4dVzBoBilZyc99Q1AABszLZBR6es7dq1y3Rbc6XrdfS+F154QU6cOGHW2fTq1UseffRRr50rAJSInAKN3kbQAQCUAbZvRlBSWLwKwG9oRcdar6NVHoKOG97PAcCebFvRAQCcocGGv2kBAMoYWzcjAAAAAFA2EXQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAwFecOCESEJB16NcAAKDICDoAAAAAbCfI2ycAAGWeVb1xreK4fl2xYumfEwAAfo6gAwDeFh7ueVtMzNmvHY5SPR0AAOyAqWsAAAAAbIeKDgB4W3Ly2elqViXnwAGmrAEAcA6o6ACAt2mgsY68bisrXeHoPgcAKAYEHQAAAAC2w9Q1APAVWr3xxcYDpdUVju5zAIBiRNABAG/TD/NW5zVdr+NrH+hLqysc3ecAAMWIqWsAAAAAbIeKDgB4i79M1SqtrnB0nwMAFCOCDgB4i79M1copaJRER7jSeh4AQJnA1DUAAAAAtkNFBwC8xd+mapVWVzhf7T4HAPArBB0A8BamagEAUGKYugYAAADAdmwZdMaPHy8BAQFuR+PGjZ33p6amypAhQ6RKlSoSHh4uvXr1kgM6XQQAvMGaqqUH1RwAAIqFLYOOatasmezbt895LF682Hnfgw8+KF9++aXMmjVLfvzxR9m7d6/ceOONXj1fAAAAAMXHtmt0goKCJDY21uP2xMREeeutt+SDDz6Qyy67zNw2ffp0adKkiSxdulQuuugiL5wtAAAAgOJk24rO5s2bJT4+XurVqyd33HGH7Nq1y9y+cuVKOXXqlHTv3t05Vqe11apVS5YsWZLr46WlpUlSUpLbAQAAAMA32TLodOjQQWbMmCHffPONTJ06VbZv3y6XXnqpHD9+XPbv3y8hISESHR3t9j0xMTHmvtxMmjRJoqKinEfNmjVL4ScBAAAAUBS2nLp29dVXO79u0aKFCT61a9eWjz/+WMqXL1+kxxw7dqyMGDHCeV0rOoQdAAAAwDfZsqKTnVZvGjZsKFu2bDHrdtLT0+XYsWNuY7TrWk5reiyhoaESGRnpdgAAAADwTWUi6CQnJ8vWrVslLi5O2rZtK8HBwbJgwQLn/QkJCWYNT8eOHb16ngAAAACKhy2nrj300ENy/fXXm+lq2jp63LhxUq5cObntttvM+pp77rnHTEOrXLmyqcwMGzbMhBw6rgEAAAD2YMug89dff5lQ8/fff0u1atWkU6dOpnW0fq2ef/55CQwMNBuFaje1Hj16yGuvvebt0wYAAABQTAIcDt2KG4WlzQi0OqT78rBeBwD8F+/nAGBPZWKNDgAAAICyhaADAAAAwHYIOkBBnTghEhCQdejXAAAA8FkEHQAAAAC2Y8uua0Cxsqo3rlUc168rViz9cwIAAECeCDpAfsLDPW+LiTn7NY0LAQAAfA5T1wAAAADYDhUdID/JyWenq1mVnAMHmLIGAADgwwg6QH5yCjR6G0EHAADAZzF1DQAAAIDtUNEBCkorODQeAAAA8AtUdAAAAADYDkEHAAAAgO0QdACUHO1UFxCQdbhusgoAAFDCCDoAAAAAbIdmBACKn1W9ca3iuH5Na24AAFDCCDoAil94uOdt1mariu51AACghDF1DQAAAIDtUNEBUPySk89OV7MqOQcOMGUNAACUGoIOgOKXU6DR2wg6AACglDB1DQAAAIDtUNEBUHK0gkPjAQAA4AVUdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYji2DzqRJk6R9+/YSEREh1atXl549e0pCQoLbmK5du0pAQIDbMWjQIK+dMwAAAIDiY8ug8+OPP8qQIUNk6dKlMn/+fDl16pRceeWVcuLECbdxAwYMkH379jmPZ555xmvnDAAAAKD4BIkNffPNN27XZ8yYYSo7K1eulM6dOztvr1ChgsTGxnrhDAEAAACUJFtWdLJLTEw0l5UrV3a7/f3335eqVatK8+bNZezYsXLy5MlcHyMtLU2SkpLcDgAAAAC+yZYVHVeZmZkyfPhwueSSS0ygsdx+++1Su3ZtiY+Pl7Vr18ro0aPNOp7Zs2fnuu5nwoQJpXjmAAAAAIoqwOFwOMTG7rvvPvn6669l8eLFUqNGjVzHLVy4UC6//HLZsmWLnH/++TlWdPSwaEWnZs2aploUGRlZYucPAChZ+n4eFRXF+zlQwD8gp6ene/s0UEYFBwdLuXLlCjze1hWdoUOHyty5c+Wnn37KM+SoDh06mMvcgk5oaKg5AAAAyiINONu3bzdhB/CW6Ohos8ZeOyaXyaCjRaphw4bJZ599JosWLZK6devm+z2rV682l3FxcaVwhgAAAP712Uo71Opf03VGS2BgmVjmDR/7HdT19AcPHizwZ3ZbBh1tLf3BBx/I559/bvbS2b9/v7ldpyaUL19etm7dau6/5pprpEqVKmaNzoMPPmg6srVo0cLbpw8AAOBTMjIyzIdMXdusXWsBb9DP8UrDjnZUzm8amy2DztSpU52bgrqaPn269OvXT0JCQuT777+XF154weyto3+Z6NWrlzz66KNeOmMAAADfdfr0aXOpn6EAb7KCtu6TWSaDTn79FTTY6KaiAAAAKLiCrIsAfOV3kAmWAAAAAGyHoAMA8D/a9enPP0XefltkxAgt5Xv7jAAAPsaWU9cAADZz6JDIsmUiS5dmXf72m26Ac/b+Bx4QqV3bm2cIAH45Deyzzz6Tnj17ih0RdAAAvkU3Z16zJivUWMFm2zbPcbogtX173QhNpBAbyAEoG/JbyzFu3DgZP358qZyLNsjKaX24LqgPCir5j+Pjx4+XOXPmOLdTsWjL8EqVKoldEXQAAN6jU8527HCv1vz+u+5M6Dm2SRORiy7KCjZ62ayZSCl8QADgn/RDvGXmzJny+OOPS0JCgvO28PBwt0ZW2lmuJEPHgAED5IknnnC7rTRCTl504007Y40OAKD0HD8usnChyFNPifzjH/qvrEi9eiK33Sby4otZYUdDTpUqItdeKzJxosh334kcPXp2Tc6994q0bEnIAby9eWN6hleO/Lrrun6Itw7dS1ErPNb1jRs3mr0Wv/76a2nbtq2EhobK4sWLzTYk2adxDR8+3G3LkszMTJk0aZLZkF73dWnZsqV88sknBWqL7HpOVsjQx9bncKXnoOdiqVOnjjz11FNy9913m/OuVauWvPnmm27f89dff8ltt90mlStXlooVK0q7du1k2bJlMmPGDJkwYYKsWbPGvAZ66G1Kv9ZKj+WPP/6Qyy67zPxcutfkwIEDJTk52Xm/9fr897//NRt26hjdv1IrU76IfyUAACVD993QcOJarVm/3rNxQHCwSKtW7tUaDT+0sQV8Vsqp09L08W+98tx/PtFDKoQUz0fYMWPGmA/t9erVK/AULg057733nrz++uvSoEED+emnn+TOO++UatWqSZcuXaSkPPfcczJx4kT597//bYLVfffdZ56vUaNGJozo1+edd5588cUXJkT9/vvvJpTdcsstsm7dOvnmm2/MPpJKg192urdkjx49pGPHjrJ8+XKzKWf//v1l6NChzmCkfvjhBxNy9HLLli3m8Vu1amUqVr6GoAMAKB4HDpwNNXosXy7i8pdAJ20aoGHGCjatW4uEhXnjjAGUcTqV7Iorrijw+LS0NFNZ0cCggUBpSNJq0BtvvJFn0Hnttddk2rRpzuv33nuvCS8Fdc0118jgwYPN16NHj5bnn3/ehA0NOh988IEcOnTIBBSt6Kj69eu7TdPTaXJ5TVXTx0hNTZV3333XVITUK6+8Itdff71MnjxZYmJizG0aCPV23ayzcePGcu2118qCBQsIOgAAm0hNFVm1yr1ao2ttstM58NowwAo1eth8TjhQFpQPLmcqK9567uKi07sKQysYJ0+e9AhH6enp0lr/aJOHO+64Qx555BHn9ejo6EI9d4sWLZxfW9PwtOqitMmAPr8Vcopiw4YNZhqeFXLUJZdcYqpCurbJCjrNmjUzIcei1R2d8uaLCDoAgLzpVDPtemYFGr3Uzj3Z52TrVDNtEGBNP9PLpk3piAbYkH7QLq7pY97k+qFeBQYGeqwBcl1/Yq1XmTdvnpkm5krX+eRFp4u5VlkK+pyWYJ3mm+2/gYYQpWtqSktwHufha/z/NxQAULwSE7P2qXGt1hw+7DmuenX3UKOVm8hIb5wxABQLXWej61lcabXE+nDftGlTE2h27dpVbOtx9DldO8Rp9zc9h27duhWq2qPT4o4cOZJjVSckJMQ8bl6aNGli1uLoWh0rAP7yyy8miOn0OH9E0AGAsiwjI6tBgGu1ZuNGz4YBISEibdq4B5s6dWgYAMBWtOPYs88+a9ap6BocbTqgocOalqYdzx566CF58MEHTRWjU6dOkpiYaAJBZGSk9O3bt0jPOWLECFMlOv/882XKlCly7NixQj2GdlvTtUPaEU2bJeh0slWrVkl8fLz5ObRr2/bt201oq1Gjhvk5slegdGqd7i2kP4Puu6NrfoYNGya9e/d2TlvzNwQdAChL9K+GrqFGGwacPOk5TrueuYYa7YqWz7QMAPB32nXssccek1GjRpmF+drOuU+fPm5rULTzmVZhNFBs27bNrLVp06aN6YZWFPoc2vpZn0cbBmiIKkw1x6rYfPfddzJy5EjTtCAjI8NUn1599VVzf69evWT27NnmcTVETZ8+3a19tdX++ttvv5UHHnhA2rdvb67r92nw8lcBjoI2I4ebpKQkM9dSU7wmeADwOSkpWZtvugab3bs9x0VEnG0UoMHmwguzpqWVEbyfA/nTD/1aEdC9Y8Lokgg/+V2kogMAdqB/s9qyxT3UrFmTNTXNVWCgSPPmZ0ONHo0bZ90OAICNEHQAwB8dPZrVMMAKNnocOeI5Tls5u27Eqa1UteUzAAA2R9ABAF+nVRmdH25txKmhJiHBc5yuoWnb1j3Y1KxJwwAAQJlE0AEAX/PXX+6tnVesyFpvk53ux+AaanQzOe2OBgAACDoA4FXa8WzlSvdqzZ49nuOioty7oGnDgKpVvXHGAAD4BYIOAJQW3Tl60yb3as3atbo7nPu4cuVELrjAvVrTsGGZbhiQmemQwyfSZO+xVNlzNEX2HkuRPWcO/frT+y6WsOBy3j5NAIAPIegAQEn5+++zjQI02GjzgJw2gYuPP9sBTYONrrM5syt1WZF66rTsS0zNCjBHzwYY63JvYqqkZ2Tm+v37E1OlTtWy9ZoBAPJG0AGA4pCenlWdca3WbN7sOa58ec+GATVqiJ3pdm1HT54ygeWvM9UY1xCz51iqHE5Oy/dxAgNEYiLDJD66vDnOM0fW9aoRbGYKAHBH0AGAouxZoxtvuu5ZoxtzpqZ6jm3UyH1tjU5JCw4WO9FKy4Gk1KypZFaQSXQNNamScirb9LwclA8uJ+dVskJMmAkyrqEmNipMgsuV3el7AIDCIegAQH6Sk7M6n7lWa/bt8xxXqZJnw4DKlcXfJaZkVWPc18XoWpmT5vLA8VST/fJTNTzUBBlThYkq7xJqso7oCsESQCtsAH6mTp06Mnz4cHMUxIwZM8zYYzlNZS4Efb/87LPPpGfPnuf0OHZG0AGA7A0DNm50r9asW5d1u6ugIJGWLd2DTYMGfrdnzelMh6nGZF/cn1WZybr9eFpGvo8TEhR4pgKTc4jRagzNAgD4uv/85z8yb948Wb16tYSEhJxzGIF3EXQAlG2HDnk2DEhK8hynG29aoUaPNm2y1tv4uBNpGR4hxupcptf3J6WasJOfShWCs8JLVFaAqXEmyFhhpkrFEAnURTQAkJcTJ0TCw89Wy0u58UrXrl2lX79+5shJenq63HzzzdKxY0d56623SvXcUPwIOgDKVsOA1avdqzXbtnmOq1BBpH1792qNdkbzxZbLyWlnp5Idy5pK5lwrk5gix06eyvdxggIDTMXFqr6Y8OJSkdEqTYUQ/rkAYH8TJkxwTi8rqilTpsj06dNl27ZtUrlyZbn++uvlmWeekXAr4J0xZ84cefjhh2X37t3SpUsXmTZtmtTUP6qd8fnnn5vz+fPPPyU+Pl769u0rjzzyiATpjIIcAtqIESPk008/laNHj0pMTIwMGjRIxo4dK2UZ/3IBsCddNLJzp2fDAA072TVp4l6tadYsa2qaD7RctiowpmOZsyKTVY3ZdyxV0k/n3nLZEhEWlEuI0XBTQapFhEo5qjEASrqS43qZ/WsbtdQPDAyUl156SerWrWvCzuDBg2XUqFHy2muvOcecPHnSTJN79913zRQ5HXPrrbfKL7/8Yu7/+eefpU+fPuZxLr30Utm6dasMHDjQ3Ddu3DiP59RxX3zxhXz88cdSq1YtE552a9OcMs77/5IDQHE4flxk+XL3YHPwoOe4KlXcWztr5SY62mstl3PaM8a6PJycQyjLpeWyW4cya8H/meuRYfbq8gbAD2WrZhgxMWe/LkhHEz/h2pRAGxU8+eSTprriGnROnTolr7zyinTQf4tE5J133pEmTZrIb7/9JhdeeKGp5IwZM8ZUcVS9evVk4sSJJjDlFHR27dolDRo0kE6dOpkmBbVr1y6Vn9XXEXQA+J/Tp0U2bMgKM1awWb/e8x9KbePcqpV7sKlXr1QaBmjLZd3EMqcQY32deiqzwC2XndUYrcKcWSujlxpyaLkMADl76qmnzGFJSUmRpUuXytChQ5236dQwrYIUl++//14mTZokGzdulKSkJMnIyJDU1FRTxamgU6NNP5sgaa9/aDujcePGEh0dLRs2bDBBZ82aNaa6o1Ufy+nTpz0ex6Jrjq644gpp1KiRXHXVVXLdddfJlVdeKWUdQQeA7ztwwL21szYM0EWs2elfsFxDTevWImFhJVKNSUrNcNszJntl5uDxtAL9gVKnjZnF/WfWwrhWZnTBf1R5Wi4DsAHrPVunq1mVHH1vL+Epa1pJ+de//uW8fscdd0ivXr3kxhtvdN6m61+Ky44dO0zIuO+++0xI0TU6ixcvlnvuuceso8keUHKTnJxsqjqu52kJy+HftTZt2sj27dvl66+/NkFLf+bu3bvLJ598ImUZQQeAb9FNN62GAVaw2bHDc5z+46j71FjBRo/Y2GI5hYzTmSaouFVhXDa/1OvJhWi57Gy77NJuWb+Oiw6T0CBaLgMoA3IKNHpbCQcdDRp6WMqXLy/Vq1eX+vXrl8jzrVy5UjIzM+W5554za3WUrpvJTqs8K1asMNUblZCQYFpZ6/Q1K7jobYU5z8jISLnlllvMcdNNN5nKzpEjR9x+/rKGoAPAe7TkoV3PXKs1q1bp5GX3cVrRaNrUvVqj18uVO6eWy26L+8/sG1OYlsuVK4a4VWGyL/jXlstUYwDAf+haFw0HeqlTxXQ/HaWBI3vXtJzoOF1/8/LLL5tuazr97PXXX/cYFxwcLMOGDTNNBHQam06lu+iii5zB5/HHHzeVIZ1Sp6FFQ5NOZ1u3bp1Z85NTp7e4uDhp3bq1GTtr1iyJjY010+HKMoIOgNKTmJg17cw12Bw+7DmuenX31s46jzkystAtl103v9xzpnOZ3paYUrCWy1pxsdbCuIYYK9SUD6EaAwCFohUcH248oAFDGwNYNDioH374wezBk5+WLVua0DF58mTT2rlz585mvY52UHOlU9hGjx4tt99+u+zZs8d0VnPdt6dHjx4yd+5ceeKJJ8xjaTDSdTz9+/fP8XkjIiJMC+vNmzdLuXLlzPqfr776yllVKqsCHDrZvAisbhGaeK+++mq3BU/6H07/o9iZLi6LioqSxMREUyoEkE1GRlaDANdQow0Esr/lhIRkraVxrdbUqZNrwwCr5fLZaWWpbmtlCtpyOTIsKMeNL61LWi6XHbyfA/nTRfC6BkRbJue0RgTwxd/FIld0dJGVdn1o166dPPTQQ3L55ZfL888/b+6bP3++7YMOgGz27XNv7bxihfseCRbteuZardGuaKGh5i79u8uRE+myd0+S2fzSWYU5s/mlXv59omAtl2Mjwzw2vjxbkQmTCFouAwBga0UOOsuXLzdzBZVucqRt7e68805T7itikajUvfrqq/Lss8/K/v37TalR51NacyMB5CElJWvzTddqza5dnuMiItwaBqS3bS/7QyPdp5V9mZAVYgrRcrlCSLms4JJLiNGQE0TLZQAAyrQiBx1tkWfRstGHH34oQ4YMkRtuuMHtPl81c+ZMGTFihFkgpps1vfDCC2Y+pHa40G4cAM7QP1xs2eJerdE/cujUNNdhgYGS2bSZJLVsI3sbt5QtdZrJhqh42X08PWta2R8pcvDXlQWaml39TMtl5/4xUbp3TAXnwn9aLgMAgBILOjovTvuC6w6sSj906I6vumPrvHnzxNfpQrEBAwbIXXfdZa5r4NHzfvvtt81OtNmlpaWZw3VON2BLR496Ngw4csRj2MlKVWVHgwtkfc0msiymgfwYWUcOOc5MB9PtEtbpgv+dHt8XeqblsuuaGBNgzoSa2Kj8Wy5v3ixy/Hju92shqUGDIvzsAACgbAYdnZJm/RX1vffey/EvqtrxQaex+TKtOGmfc+2GYdGuFLqx0pIlS3L8Hu2YoRs3AbaiVZk//jCB5tSvS8SxdKmEbNnsMSwtKFjWxdSXVXENZVV8Y1kd30j2RFZzbxhwplKjLZWd4SU6qwrjuuD/XFsua8hp2DD/cZs2EXYAACjLChV0/vnPf5opX7rZUl6bDzXV/S182OHDh01v9BhrZ94z9PrGjRtz/B4NRTrVzbWiU7NmTfEZuujb6u+uuw+X8AZc8C/acvlQcpoc3LhV0hb/KkHLl0v02pUSt2W9hKZnVSpdl+ZvrxQnq+Mayap4PRrLxup15FS5YAkuFyBxUeWlZnSYXBRdQc6zNsG0gkxUybdczquSU5RxAADAngoVdP7880/TD1yneGVfx6I7vE6bNk0GDRokdhQaGmoOwBelpJ82C/qtDmV/bEuRwwePSnTCKqmxZbU02LleWu7ZKBck/+3xvUmhFWW1qdQ0kk11msnhpi0lokasc4F/e5e1MtXCQyXQyy2XdVpacY4DAAD2VKigs3TpUlPV0cX7uglRkyZNJDMzU6ZPny4TJ040Gx75Q9CpWrWq2UzpwIEDbrfrdd1F1q9Y7Xtd2/i6fk1lx+/plFFtqWwW9B9Lkb/MnjFnN7/UyyPJqVLvyB5ptXeTtN67UW7bt0kaH9wuQQ73DmYZAYGyIfp8WRHZUsKvaiOhXS+UiJbN5LzKFeXuKP9ouazT0XRaGmt0AABAsQUdDQi6M2zfvn3lkksukZEjR8qMGTPMpj233HKLPPbYY+IPQkJCpG3btrJgwQLp2bOnuU0Dm14fOnSo+BVrupor1yl5ftLquyxLz8iUfc72yu6bX+rXentahntgiU5JMqGm+94EabUvQVrtTZCoNM89a/YGxciyoPayLPMi+TX9ElnpaC8nj1YUOSqycqBImzbilwgxAACg2LuuaUjo1auXfPnll/L444+bwDB37lxp1KiR+BNdb6OBTTc81b1ztL30iRMnnF3YgOKqxiSlZMhfx066VWHMcSbQ6NqZvPJo0OkMueDwDun89xZpv3+zNNm9QWL2e+5Z4wgLk4B27eRoo4tkwFsdZKlcJHsyaoi4d4F2YmoXAAAFo422jh07JnPmzCnQ+B07dpgOxatWrZJWujF2EXXt2tV8v35ORQkHnVmzZsmTTz4p69atk3/84x9y/vnny0svvSSLFi3yu6CjFahDhw6ZsKYbhuov0TfffOPRoMDnaeMBa7qade46JY8pa6Ui43SmHDie5gwtbhthnrntRPrpfB8nLDgwa1F/VJg0zUiUlnsTpP62dRK7ca2Er18jgampnt+krcfObMSplwEXXCASHCyVtEvgaKZ2AQCQk9mzZ5ttRbQD75EjR845jMAmQUfDwTXXXGPW5LQ5M+elRYsWMnDgQNmyZYs8++yz4k90mprfTVXLLqdAo7cRdIpFclpGjiHGCjL7k1IlswCzA7WlsulMFnW2Q5l2LKsZnCk1tq2XyDW/S8DiZVl71uzb5/kAlSo5A425vPBCkTw6HxJiAADImc7g0X0g//Wvf5k9FWFfhW5GoNO8XPXp08e0WdbpbLpW55NPPinucwRKtOVy1uL+s2Hm7IL/FElKzWXelwttuWy1VrZaLTvbLp/ZOyYsuJw+oYi2L9cw8+XSrM04163Lut1VUJD+BcGtWmOSyznsPQMAALL07t3bOb2sqHQWkDXLSRtcdezYUV588UUz28mVblsyePBg+f3336V+/fry6quvSpcuXZz36/c//PDD8vPPP0vFihXlyiuvlOeff96si8/Ja6+9Zu7fvXu3REVFyaWXXspn7+IKOtlDjqVbt27yyy+/yHXXXVeYh0Nx0goOjQdybLmc47SyYymyPzFVTp3O/zWLKh/sDCxZG1+eDTF6VM2t5fKhQyLLFmUFGw01v/2mGzB5jtP9mFyrNVotrVChmF4FAAC8Tzd7ttOUaq0K6XpvndmUnJxslkLccMMNsnr1arMJvUVDjK6v0T0mp0yZItdff70pDFSpUsWs+bnsssukf//+JrykpKTI6NGjTaVp4cKFHs+5YsUKuf/+++V///ufXHzxxWbanQYkFGMzgtxoq2mt+ACl2XI552llqebrIyfS832ccoEBEhsZdibIhJ3d+FJDTXR5iYsuL+GhBfi/SXq6yOrVZ0ONXm7d6jlOA0y7dmdDjR7nnVfEVwEAAP8IObqsND+6dYC/hB2dyeTq7bfflmrVqpk9J5s3b+68XZdIWGOnTp1qKkFvvfWWjBo1Sl555RVp3bq1PPXUU26PozOlNm3aJA2zvWi7du0yVR8tLEREREjt2rXN96MUgo7S/8BAcUjLOG0qLlZ75azw4t65LHvL5ZxoSLFCzNlpZdYamfJSPSJUgsqd/ctLgWjlbOfOs4FGL1etEklL8xzbpIl7tUbf/HRqGgAAZURelZyijCuo999/X+69917n9a+//tpM9SoOmzdvNlWcZcuWyeHDh802JVYYcQ06OqXNEhQUZLr9btiwwVxfs2aN2bYlPIetQrZu3eoRdK644goTburVqydXXXWVObSKVIFZILniExe8Uo1JTDnl1mJ5r1uoSZGDx3MIDdnokhUNKs7gYoUYlwX/kWFBEnCua1v0nXf5cvdgc/Cg57gqVdzX1bRvLxIdfW7PDQCAnyvodgbFve2BdgjWTe4t5xXjDAqdgqah4//+7/8kPj7eBB0NOOk6w6OAdMqbPs7kyZM97ouLi/O4Tas4utZHux1/9913JmiNHz9eli9fLtF83sgRQQcl0nJZu5G5VmFcF/wXpuWya/XF+tpaKxMTGSYhQYWsxuTn9GkR/UuLa6hZv95z/ZNWZbRc7Fqt0QWINAwAAMCNTkfTaWmlvUZHg4Eexe3vv/+WhIQEE3KsCtHixYtzHKvLOjp37my+zsjIMC2trY6/2sH4008/lTp16phqT0HouO7du5tj3LhxJuDoep4bb7yx2H4+OyHooNCOp55yhpg91lQylyBT0JbLVcND3MKLW6ipVF4qVQg+92pMfnTPIdd1NVq5yemduHbts6FGDw05YWEle24AANiEL6290UX8OsVs79695rqGFhUbG2uO/FSqVMk0E3jzzTdN5UUfa8yYMTmO1S5rDRo0MGvZteHA0aNH5e677zb3DRkyxISl2267zazZqVy5stmu5aOPPpJp06aZbm6u5s6dK9u2bTPBSc/hq6++MpUkf9vLsjQRdODmtLZc1g0wc9gzxrpekJbLIeUCJU4X9+dQkdEQExcVltVyuTTpGhpdS+NarcmptaR2sNMOg67VmthY944xf/p/xxgAAMqiL774Qu666y7n9VtvvdVcaoVEp4LlR7uqaRjRDmg6XU2DxksvvSRdu3b1GPv000+bQ7uxaXtpfW6rdbROedOuxdppTdtKp6WlmelwuvbGtXObRas3utmpnmNqaqoJUB9++KE0a9bsHF8R+wpw6IIJFFpSUpLpX56YmCiRkZHiTy2XXVssOzuWaUUmseAtl6MrBOcyrSyrc1nVirm0XC4t+mu9bZt7tUZDzqlT7uO0YtS0qfvaGr2e7a8oduwYA8C/38+B0qQfrLUtct26dSWMGQ3wk99FKjo2opn1cHK62+aXriFGp5sVquWys0OZVmYqnLnMCjQVC9JyuTQlJno2DDh82HOcdgbM3jCgAB9svNUxBgAAAEXjY59WkV/L5X1n1sT85TqtzGXvmPRCtFzO6kzmHmL0tuoRYSbs+KyMjKwGAa7VGm0gkL04GRKStZbGNdjUqVOkhgHe6hgDAACAoiHo+FA15tjJUx7TykzHsjNf69qZ/Ohn+JgI140vw8zGl84F/5W05XKw+JV9+zwbBpw44Tmubl33UNOqlUhoqF93jAEAAEDREHS8ZPeRk/Laoq1uoeZkAVoulw8ud2YdTAU5TzfBdNkzRisysVFhElzYDTB9SUqKyO+/uwebXbtyThXaMMAKNnpUr16ip0aIAQAA8B8EHS9JP50pH/7m+QG+arhugHmmIpMtxOgRXRotl0uLTjXbsuVsqNFjzZqsqWmutPOIdhRxrdY0buzRMAAAAACwEHS8REPLA5c3cFnw76WWy6Xp6FGR335zr9YcOeI5Libm7H41GmzatWPxCwAAAAqFoOMlGmgevKIA/Yr9lVZl/vjDvVpzZkMuN7qGpk0b92pNrVpFahgAAAAAWAg6KB579ri3dl65UuTkSc9x9eu7b8TZsmVWdzQAAACgGBF0UHgaYDTIuAYbDTrZRUWdbRSgwUabB5zZDRgAAAAoSQQd5C0zU2TzZvdQs3atyOnTng0DWrRwr9Y0apR1OwAAgI/p2rWrtGrVSl544YVie8zx48fLnDlzZPXq1eLPupbAa+NqxowZMnz4cDl27JiUJIIO3P39d1bDACvY6JHTL2Fc3NmGAXq0bStSsaI3zhgAACBH/fr1k3feecfj9s2bN8vs2bMlOLh09xbcsWOH1K1bV1atWmWCRGmGi5wsWrRIunXrJkePHpXo6GgpLbfccotcc801Jf48BJ2y7NSprOqMa7VGqzfZhYVldT5zrdbUqEHDAAAA4POuuuoqmT59uttt1apVk3JsU+E15cuXN0dJY15RWaF71ujGm7NmiYwcKdKpk0hkZFaAGTpU5H//OxtyGjYU6dNH5NVXs9biJCWJ/PyzyH//K3LTTSI1axJyAAAo658rTpzwzqHPXQihoaESGxvrdmjI0QqKTp+y1KlTR5566im5++67JSIiQmrVqiVvvvmm22ONHj1aGjZsKBUqVJB69erJY489Jqf0D8fF7IknnpDmzZt73K4VH31Oq1rVs2dPmTBhgglukZGRMmjQIElPT3eOT0tLk/vvv1+qV68uYWFh0qlTJ1m+fLmzuqTVHFWpUiWzT6M+piUzM1NGjRollStXNq+ZTstzpdPO+vfv73zuyy67TNbofohn6Nf6+Ppa6v1t27aVFStWOKeuuVaQ8hp7Lqjo2JW+EegviGu1Zt8+z3GVKnk2DKhc2RtnDAAA/KkxUXi4d547ObnEpss/99xzMnHiRPn3v/8tn3zyidx3333SpUsXaaTrjkW39YswH9Lj4+Pljz/+kAEDBpjbNBAUJw1bGmA0lLRv397cptPd1q5da6bcWRYsWGACjE5B0+By1113SZUqVeQ///mPuV/P69NPPzXT92rXri3PPPOM9OjRQ7Zs2SI1a9Y09/Xq1UsSEhJMwHCtsuj3jBgxQpYtWyZLliwxIeiSSy6RK664wtx/8803m/Fff/21REVFyRtvvCGXX365bNq0yYSjO+64Q1q3bi1Tp041wVLXLeU2VbAwYwvFgSJJTEzUPyeYS687fdrhWL/e4Xj7bYdj4ECHo2VLhyMwUP/e4X6UK+dwtGnjcAwe7HC8847DkZDgcGRmevvsAcCrfOr9HPBRKSkpjj///NNcGsnJnp8zSuvQ5y6gvn37OsqVK+eoWLGi87jpppvMfV26dHE88MADzrG1a9d23Hnnnc7rmZmZjurVqzumTp2a6+M/++yzjrZt2zqvjxs3ztFSP4flYvv27eb9pnz58m7npEdgYKDb+Vx99dWO++67z3l92LBhjq5du7r9bJUrV3acOHHCeZuea3h4uOP06dOO5ORkR3BwsOP999933p+enu6Ij493PPPMM+b6Dz/8YM7n6NGjbuepr02nTp3cbmvfvr1j9OjR5uuff/7ZERkZ6UhNTXUbc/755zveeOMN83VERIRjxowZOb4O06dPd0RFRTmv5zU239/FPFDR8UeHD5+t0uilNg9ITPQcp+toXDfi1I05K1TwxhkDAAA70c8TWlnx1nMXgk6J0kqBpWIe1aAW2kH2DJ3KpVO2Dh486Lxt5syZ8tJLL8nWrVslOTlZMjIyTCWksPRxmjRp4lHVcKXVIq3sTJkyRQIDA+WDDz6Q559/3m1My5YtzTQ6S8eOHc157d69WxITE820Oq3CWLRKcuGFF8qGDRvyPUfX10LFxcU5XwudaqbPo9UjVykpKea1UVoN0qlt//vf/6R79+6mAnT++efn+FyFGVsYBB1fp/MstUWha7A58wvkRn/Jdb2NFWz0OO88KUt0idHx47nfHxEh0qBBaZ4RAAA2pWt1/aTbqgab+rpheQFkny6lYUfXqiidvqVhRKeU6fQvna710UcfmeluhaXTxrKfU/bF+ddff71ZX/TZZ59JSEiICS036VrpUhKcx2uhIUeDj06Zy85ae6Nrem6//XaZN2+emd42btw483rdcMMNHt9TmLGFQdDxJVqQ3bnTfV3NqlW6ksxzbOPGZ1s7a6jRBWtBZfc/p4Yc7aGQn02bCDsAAKDwfv31V7PO5ZFHHnHetlM/t5WQoKAg6du3r+kYp0Hn1ltv9QhDWlnRKop1+9KlSyU8PNwEqapVq5rv++WXX8x5Kw1Luu7HasKg96vT2fdHzEebNm1k//795hy1iUNutHGDHg8++KDcdttt5mfJLbwUZmxBld1Pxr5Ayw/a+cK1WnPggOc4LQu6tnbWhgGl2OvcH+RVySnKOAAAAFcNGjSQXbt2mUqDNgjQ6oNWW0qSTueyprhpYMlOO6zdc8898uijj5pmBFoJGTp0qJnqVrFiRdNM4eGHHzbNAbSLnDYjOHnypPkepQFIKzVz5841+9poYNKglB+dXqbT5LTrmz6mBpS9e/ea10TDSbNmzczzagVK9w3666+/TMDSxgfZaVAr6NjCIuh4i7Zr7tLFs0WiVmV0AynXtTU6R5F2znnSaWnFOQ4AAMDVP/7xD1Nt0CChbZuvvfZa0+o5e9vl4g5XF198sRw5ckQ66OfCbLTLmY7p3LmzOSethLiez9NPP22mm/Xu3VuOHz8u7dq1k2+//da0k1bnnXeemYo3ZswY07GtT58+pqtcfjQcffXVV6a6pd936NAhs55JzyMmJsZ0Tvv777/N4x04cMBUl2688UbzXNkVZmxhBWhHgnN+lDIoKSnJzM3UhV5FWYQmR49mtXHWUqJrtaZ1a52kWRKnbHus0QHglfdzoAxITU2V7du3m7+4aztjlA79mK5BZvDgwWbBvitt96x72cyZM0fKktRC/C5S0fEWTdLauaJaNW+fiW0QYlDaCNcAgJKiVRKdJqdrYbRqgsIj6HgTIQfwWzTAAACUpOrVq5tpXG+++aZzqhkKh6ADAEVAAwwAQEnKb3VJQdbSlHWB3j4BAPBHNMAAUBaxtBv+9DtIRQcAikCno+m0NNboACgLtDOW1c44+14uQGnS9tg5bWhaJoKO9hCfOHGiLFy40Czeio+PlzvvvNO0v7M2RdIx2qkhO93x9iLtfgYABUCIAVBW6MaQFSpUMAvk9QOm7tMClHYlR0POwYMHJTo62hm+y1TQ2bhxo+kX/sYbb0j9+vVl3bp1MmDAADlx4oT897//dRv7/fffmw2NLFV0Y04AAAB47JsSFxdn2vru3LnT26eDMiw6Otrs2VMQtgs6V111lTks9erVk4SEBJk6dapH0NFgU9AXSjdh0sN13wUAAICyQmfG6J4uOn0N8AatJhakkmPboJMT3QSusm7OmcMOt7rpUMOGDWXUqFHmem4mTZpULDu0AgAA+CudssaGofAXtp9guWXLFnn55Zfl3nvvdd4WHh4uzz33nMyaNUvmzZsnnTp1kp49e8oXX3yR6+OMHTvWBCbr2L17dyn9BAAAAAAKK8DhJ30Cx4wZI5MnT85zzIYNG6Rx48bO63v27JEuXbpI165dZdq0aXl+b58+fcy8059//rlA56NT16KiokzoiYyMLOBPAQDwNbyfA4A9+c3UtZEjR0q/fv3yHKPrcSx79+6Vbt26ycUXX2x2lM1Phw4dZP78+cVyrgAAAAC8y2+CTrVq1cxREFrJ0ZDTtm1bmT59eoFaIK5evdp0EwEAAADg//wm6BSUhhydqla7dm3TZU37vVusDmvvvPOO6RzSunVrc3327Nny9ttv5zu9DQAAAIB/sF3Q0eln2oBAjxo1arjd57ocSTcV1T7wugGWruuZOXOm3HTTTV44YwAAAABlthmBHRevbt4scvx47vdHRLDzOgCUNJoRAIA92a6i4y805DRsmP+4TZsIOwAAAEBh2X4fHV+VVyWnKOMAAAAAnEXQ8RKdllac4wAAAACcxdQ1L9HpaDotjTU6AAAAQPEj6HgRIQYAAAAoGUxdAwAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtmPLoFOnTh0JCAhwO55++mm3MWvXrpVLL71UwsLCpGbNmvLMM8947XwBAAAAFK8gsaknnnhCBgwY4LweERHh/DopKUmuvPJK6d69u7z++uvyxx9/yN133y3R0dEycOBAL50xAAAAgOJi26CjwSY2NjbH+95//31JT0+Xt99+W0JCQqRZs2ayevVqmTJlCkEHAAAAsAFbTl1TOlWtSpUq0rp1a3n22WclIyPDed+SJUukc+fOJuRYevToIQkJCXL06NEcHy8tLc1UglwPAAAAAL7JlhWd+++/X9q0aSOVK1eWX3/9VcaOHSv79u0zFRu1f/9+qVu3rtv3xMTEOO+rVKmSx2NOmjRJJkyYUEo/AQAAAIAyUdEZM2aMR4OB7MfGjRvN2BEjRkjXrl2lRYsWMmjQIHnuuefk5ZdfNlWZotKwlJiY6Dx2795djD8dAAAAgDJZ0Rk5cqT069cvzzH16tXL8fYOHTqYqWs7duyQRo0ambU7Bw4ccBtjXc9tXU9oaKg5AAAAAPg+vwk61apVM0dRaKOBwMBAqV69urnesWNHeeSRR+TUqVMSHBxsbps/f74JQTlNWwMAAADgX/xm6lpBaaOBF154QdasWSPbtm0zHdYefPBBufPOO50h5vbbbzeNCO655x5Zv369zJw5U1588UUz5Q0AAACA//Obik5B6fSyjz76SMaPH2/W5GjTAQ06riEmKipKvvvuOxkyZIi0bdtWqlatKo8//jitpQEAAACbCHA4HA5vn4Q/0vbSGpi0MUFkZKS3TwcAUES8nwOAPdlu6hoAAAAAEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDt2G7DUPiuzZtFjh/P/f6ICJEGDUrzjAAAAGBXBB2UWshp2DD/cZs2EXYAAABw7pi6hlKRVyWnKOMAAACAvBB0UCp0WlpxjgMAAADywtQ1lAqdjqbT0lijAwAAgNJA0EGpIcQAAACgtDB1DQAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2A5BBwAAAIDtEHQAAAAA2I7tgs6iRYskICAgx2P58uVmzI4dO3K8f+nSpd4+fQAAAADFIEhs5uKLL5Z9+/a53fbYY4/JggULpF27dm63f//999KsWTPn9SpVqpTaeQIAAAAoObYLOiEhIRIbG+u8furUKfn8889l2LBhpmrjSoON61gAAAAA9mC7qWvZffHFF/L333/LXXfd5XHfP/7xD6levbp06tTJjMtLWlqaJCUluR0AAAAAfJPtg85bb70lPXr0kBo1ajhvCw8Pl+eee05mzZol8+bNM0GnZ8+eeYadSZMmSVRUlPOoWbNmKf0EAAAAAAorwOFwOMQPjBkzRiZPnpznmA0bNkjjxo2d1//66y+pXbu2fPzxx9KrV688v7dPnz6yfft2+fnnn3Ot6Ohh0YqOhp3ExESJjIws9M8DAPAN+n6uf8Di/RwA7MVv1uiMHDlS+vXrl+eYevXquV2fPn26WYejU9Ty06FDB5k/f36u94eGhpoDAAAAgO/zm6BTrVo1cxSUFqo06GilJjg4ON/xq1evlri4uHM8SwAAAAC+wG+CTmEtXLjQTEXr37+/x33vvPOO6c7WunVrc3327Nny9ttvy7Rp07xwpgAAAACKW5CdmxDonjqua3ZcTZw4UXbu3ClBQUFmzMyZM+Wmm24q9fMEAAAAUIabEfgaFq8CgD3wfg4A9mT79tIAAAAAyh6CDgAAAADbse0aHZSMzZtFjh/P/f6ICJEGDUrzjAAAAABPBB0UKuQ0bJj/uE2bCDsAAADwLqauocDyquQUZRwAAABQUgg6KDCdllac4wAAAICSwtQ1FJhOR9NpaazRAQAAgK8j6KBQCDEAAADwB0xdAwAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAtkPQAQAAAGA7BB0AAAAAthPk7ROAb9m8WeT48dzvj4gQadCgNM8IAAAAKDyCDtxCTsOG+Y/btImwAwAAAN/G1DU45VXJKco4AAAAwFsIOnCbllac4wAAAABvYeoanHQ6mk5LY40OAAAA/B1BB24IMQAAALADpq4BAAAAsB2CDgAAAADbIegAAAAAsB2CDgAAAADbIegAAAAAsB2CDgAAAADbIegAAAAAsB2CDgAAAADbIegAAAAAsB2CDgAAAADbCfL2CQCAL9i8WeT48dzvj4gQadCgNM8IAACUqYrOf/7zH7n44oulQoUKEh0dneOYXbt2ybXXXmvGVK9eXR5++GHJyMhwG7No0SJp06aNhIaGSv369WXGjBml9BMA8MWQ07ChSNu2uR96v44DAAD+we+CTnp6utx8881y33335Xj/6dOnTcjRcb/++qu88847JsQ8/vjjzjHbt283Y7p16yarV6+W4cOHS//+/eXbb78txZ8EgK/Iq5JTlHEAAMD7AhwOh0P8kIYXDSjHjh1zu/3rr7+W6667Tvbu3SsxMTHmttdff11Gjx4thw4dkpCQEPP1vHnzZN26dc7vu/XWW81jffPNNwV6/qSkJImKipLExESJjIws5p8OgDcqOvnZtInpa3bE+zkA2JPt1ugsWbJELrjgAmfIUT169DAVoPXr10vr1q3NmO7du7t9n47R4JSbtLQ0c1j0H0TrH0gA/k3fLn7/Pf81OjqO/8vbj/U+7qd/9wMAlJWgs3//freQo6zrel9eY/Qfu5SUFClfvrzH406aNEkmTJjgcXvNmjWL+ScAAHjD8ePHTWUHAGAPPhF0xowZI5MnT85zzIYNG6Rx48biLWPHjpURI0Y4r2dmZsqRI0ekSpUqEhAQUKTH1GClQWn37t1MlygGvJ7Fj9e0+PGa+t7rqZUcDTnx8fHFfn4AgDIedEaOHCn9+vXLc0y9evUK9FixsbHy22+/ud124MAB533WpXWb6xj9RzKnao7S7mx6uMqt61th6fPygaf48HoWP17T4sdr6luvJ5UcALAfnwg61apVM0dx6Nixo2lBffDgQdNaWs2fP9/8A9i0aVPnmK+++srt+3SM3g4AAADA//lde2ndI0dbQuultpLWr/VITk4291955ZUm0PTu3VvWrFljWkY/+uijMmTIEGdFZtCgQbJt2zYZNWqUbNy4UV577TX5+OOP5cEHH/TyTwcAAADANhWdwtD9cHRvHIt2UVM//PCDdO3aVcqVKydz5841Xda0QlOxYkXp27evPPHEE87vqVu3rmkvrcHmxRdflBo1asi0adNM57XSpMFr3LhxHlPiUDS8nsWP17T48ZoWL15PAIDt9tEBAAAAANtMXQMAAACA/BB0AAAAANgOQQcAAACA7RB0AAAAANgOQQcAAACA7RB0SoFuYHrxxRdLhQoVJDo6Oscxui/Qtddea8boRqcPP/ywZGRkuI1ZtGiRtGnTxrRRrV+/vsyYMaOUfgLfV6dOHQkICHA7nn76abcxa9eulUsvvVTCwsKkZs2a8swzz3jtfP3Bq6++al5Xfb06dOggv/32m7dPyW+MHz/e4/excePGzvtTU1PN3l5VqlSR8PBw6dWrlxw4cMCr5+xLfvrpJ7n++uslPj7evHZz5sxxu1+bhepWA3FxcVK+fHnp3r27bN682W3MkSNH5I477jCbRev77j333OPcbw0AUDYQdEpBenq63HzzzWZvn5zoxqcacnTcr7/+avYJ0hCj/5Bbtm/fbsZ069bNbJA6fPhw6d+/v9kQFVl0r6R9+/Y5j2HDhjnvS0pKMpvJ1q5dW1auXCnPPvus+TD65ptvevWcfdXMmTNlxIgRZn+S33//XVq2bGn2mTp48KC3T81vNGvWzO33cfHixc77dA+vL7/8UmbNmiU//vij7N27V2688Uavnq8vOXHihPmd07CdE/0jxUsvvSSvv/66LFu2zOyXpr+fGiAtGnLWr18v8+fPN3uraXgaOHBgKf4UAACv0310UDqmT5/uiIqK8rj9q6++cgQGBjr279/vvG3q1KmOyMhIR1pamrk+atQoR7Nmzdy+75ZbbnH06NGjFM7c99WuXdvx/PPP53r/a6+95qhUqZLz9VSjR492NGrUqJTO0L9ceOGFjiFDhjivnz592hEfH++YNGmSV8/LX4wbN87RsmXLHO87duyYIzg42DFr1iznbRs2bND9zBxLliwpxbP0D/q6fPbZZ87rmZmZjtjYWMezzz7r9pqGhoY6PvzwQ3P9zz//NN+3fPly55ivv/7aERAQ4NizZ08p/wQAAG+houMDlixZIhdccIHExMQ4b9O/TmoVQv8iaY3R6RmudIzejiw6VU2nArVu3dpUbFyn/unr1LlzZwkJCXF7/RISEuTo0aNeOmPfpJVFrXq5/r4FBgaa6/y+FZxOpdKpV/Xq1TPVBZ2eqvS1PXXqlNvrq9PaatWqxetbAFrd3r9/v9vrFxUVZaZXWq+fXup0tXbt2jnH6Hj9PdYKEACgbAjy9glAzD/ariFHWdf1vrzGaBhKSUkx89TLsvvvv9+sX6pcubKZ/jd27FgzXWjKlCnO169u3bq5vsaVKlXyynn7osOHD5vplDn9vm3cuNFr5+VP9EO3Tj9t1KiR+T2cMGGCWR+2bt068/umgTv7ej19fa3/vyN31muU0++n6/ulrnV0FRQUZN4feI0BoOwg6BTRmDFjZPLkyXmO2bBhg9sCZJTca6zrSSwtWrQwHyTvvfdemTRpkmneAJSmq6++2u33UYOPrg/7+OOPy/wfJQAAKC0EnSIaOXKk9OvXL88xOmWlIGJjYz06WlkdmPQ+6zJ7Vya9rh2F7PrB6VxeY/1gqVPXduzYYf6qntvr5/oaI0vVqlWlXLlyOb5evFZFo9Wbhg0bypYtW+SKK64w0wOPHTvmVtXh9S0Y6zXS10u7rln0eqtWrZxjsjfO0PcD7cTGawwAZQdBp4iqVatmjuLQsWNH04Ja/2G2pltopyANMU2bNnWO+eqrr9y+T8fo7XZ1Lq+xdqbT+fjW66mv0yOPPGLWRgQHBztfPw1BTFtzp9Wwtm3byoIFC6Rnz57mtszMTHN96NCh3j49v6Rtjbdu3Sq9e/c2r63+DurrqW2lla4V0zU8dv7/c3HRKagaVvT1s4KNTuHVtTdWZ0t9HTVI6noofb3VwoULze+x/hEEAFBGeK0NQhmyc+dOx6pVqxwTJkxwhIeHm6/1OH78uLk/IyPD0bx5c8eVV17pWL16teObb75xVKtWzTF27FjnY2zbts1RoUIFx8MPP2w6NL366quOcuXKmbFl3a+//mo6rulrt3XrVsd7771nXr8+ffq4dWWKiYlx9O7d27Fu3TrHRx99ZF7PN954w6vn7qv09dEuVjNmzDAdrAYOHOiIjo526wyI3I0cOdKxaNEix/bt2x2//PKLo3v37o6qVas6Dh48aO4fNGiQo1atWo6FCxc6VqxY4ejYsaM5kEXfG633Sf1nasqUKeZrfS9VTz/9tPl9/Pzzzx1r1651/POf/3TUrVvXkZKS4nyMq666ytG6dWvHsmXLHIsXL3Y0aNDAcdttt3nxpwIAlDaCTino27ev+cc6+/HDDz84x+zYscNx9dVXO8qXL28+EOkHpVOnTrk9jo5v1aqVIyQkxFGvXj3TrhoOx8qVKx0dOnQwrbvDwsIcTZo0cTz11FOO1NRUt3Fr1qxxdOrUyXyAP++888yHJeTu5ZdfNh/G9fdN200vXbrU26fkN7T1e1xcnHnt9HdNr2/ZssV5v34gHzx4sGl5roH7hhtucOzbt8+r5+xL9L0up/dMfS+1Wkw/9thj5o8X+v/nyy+/3JGQkOD2GH///bcJNvrHJW3Vf9dddzn/uAQAKBsC9H+8XVUCAAAAgOLEPjoAAAAAbIegAwAAAMB2CDoAAAAAbIegAwAAAMB2CDoAAAAAbIegAwAAAMB2CDoAAAAAbIegAwAAAMB2CDoAAAAAbIegA0CWLVsmAQEBMnPmTOdthw8flvr168t1110np0+f9ur5AQAAFFaAw+FwFPq7ANjOtddeK7t27ZK1a9dKenq6XHbZZZKSkiI//fSThIeHe/v0AAAACoWgA8BYuXKltGvXTj7++GOZPXu2/Prrr7J06VKJi4vz9qkBAAAUGkEHgFPPnj1l/vz5EhwcLIsXL5bmzZs777v33ntl7ty5snfvXuFtAwAA+DrW6ABw0jU5J0+elJEjR7qFHHXHHXfI77//7rVzAwAAKAwqOgCMd999V+655x5p2bKlHD16VBISEiQoKMhjnDYt4G0DAAD4Oio6AOSHH36QAQMGyIsvvijvvfee7NixQ6ZPn+7t0wIAACgyKjpAGbdhwwa5+OKL5a677pIpU6aY226//Xb55ZdfZPPmzRISEuI2nooOAADwBwQdoAw7ePCgXHTRRWa62qeffiqBgYHO8KNrdF5++WUZPHiw2/cQdAAAgD8g6AAoFIIOAADwB6zRAVAg/fr1kxo1apiv9bJ3797ePiUAAIBcUdEBAAAAYDtUdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAADYDkEHAAAAgO0QdAAAAACI3fw/HMkKwG4XM7EAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5tQh6NlDdnR"
   },
   "source": [
    "#### Exercise 1.5\n",
    "Which of the following problems are more suited for the learning approach and which are more suited for the design approach?\n",
    "\n",
    "The main difference between the learning approach and the design approach is the role that data plays.\n",
    "\n",
    "In the design approach, the problem is well specified and one can analytically derive $f$ without the need to see any data.\n",
    "\n",
    "In the learning approach, the problem is much less specified, and one needs data to pin down what $f$ is.\n",
    "\n",
    "(a) Determining the age at which a particular medical test should be performed\n",
    "\n",
    "(b) Classifying numbers into primes and non-primes\n",
    "\n",
    "(c) Detecting potential fraud in credit card charges\n",
    "\n",
    "(d) Determining the time it would take a falling object to hit the ground\n",
    "\n",
    "(e) Determining the optimal cycle for traffic lights in a busy intersection\n",
    "\n",
    "1. (a) Learning\n",
    "1. (b) Design\n",
    "1. (c) Learning\n",
    "1. (d) Design\n",
    "1. (e) Learning\n",
    "\n",
    "#### Exercise 1.6\n",
    "For each of the following tasks, identify which type of learning is involved (supervised, reinforcement, or unsupervised) and the training data to be used. If a task can fit more than one type, explain how and describe the training data for each type.\n",
    "\n",
    "(a) Recommending a book to a user in an online bookstore\n",
    "\n",
    "(b) Playing tic tac toe\n",
    "\n",
    "(c) Categorizing movies into different types\n",
    "\n",
    "(d) Learning to play music\n",
    "\n",
    "(e) Credit limit: Deciding the maximum allowed debt for each bank customer\n",
    "\n",
    "1. (a) Supervised Learning\n",
    "1. (b) Reinforcement Learning\n",
    "1. (c) Unsupervised Learning\n",
    "1. (d) Learning to play music\n",
    "  * If learn by yourself, it's unsupervised learning\n",
    "  * If learn from a teacher, it's supervised learning\n",
    "  * If learn by yourself but with someone to tell you if your music is good or not, it's reinforcement learning.\n",
    "1. (e) Supervised Learning\n",
    "\n",
    "#### Exercise 1.7\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1YT9yY7lUk7SPsjVgM60W8PGkOjCm9gip'>\n",
    "\n",
    "For each of the following learning scenarios in the above problem, evaluate the performance of the final hypothesis $g$ on the three points in $X$ outside $D$. To measure the performance, compute how many of the 8 possible target functions agree with $g$ on all three points, on two of them, on one of them, and on none of them. How many distinct functions $f$ do we have?\n",
    "\n",
    "(a) $H$ has only two hypotheses, one that always returns '•' and one that always returns 'o'. The learning algorithm picks the hypothesis that matches the data set the $most$.\n",
    "\n",
    "(b) The same $H$, but the learning algorithm now picks the hypothesis that matches the data set the $least$.\n",
    "\n",
    "(c) $H$ = {XOR} (only one hypothesis which is always picked), where XOR is defined by XOR(x) = '•' if the number of 1's in $x$ is odd and XOR(x) = 'o' if the number is even.\n",
    "\n",
    "(d) $H$ contains all possible hypotheses (all Boolean functions on three variables), and the learning algorithm picks the hypothesis that agrees with all training examples, but otherwise disagrees the most with the XOR.\n",
    "\n",
    "\n",
    "1. (a) The learning algorithm will pick the final hypothesis that always returns 1.\n",
    "  * 1 out of 8 $f$ agrees with $g$ on all three points; 3 $f$ agree with $g$ on two of the points; 3 $f$ agree with $g$ on one of the points, 1 $f$ agrees with none of the points.\n",
    "1. (b) The learning algorithm will pick the final hypothesis that always returns 0.\n",
    "  * 1 out of 8 $f$ agrees with $g$ on all three points; 3 $f$ agree with $g$ on two of the points; 3 $f$ agree with $g$ on one of the points, 1 $f$ agrees with none of the points.\n",
    "1. (c) The learning algorithm will pick the final hypothesis $XOR$.\n",
    "  * 1 out of 8 $f$ agrees with $g$ on all three points; 3 $f$ agree with $g$ on two of the points; 3 $f$ agree with $g$ on one of the points, 1 $f$ agrees with none of the points.\n",
    "1. (d) The learning algorithm will pick the final hypothesis $f_7$.\n",
    "  * 1 out of 8 $f$ agrees with $g$ on all three points; 3 $f$ agree with $g$ on two of the points; 3 $f$ agree with $g$ on one of the points, 1 $f$ agrees with none of the points.\n",
    "  \n",
    "#### Exercise 1.8\n",
    "If $μ$ = 0.9, what is the probability that a sample of 10 marbles will have $v$ <= 0.1? [Hints: 1. Use binomial distribution. 2. The answer is a very small number.]\n",
    "\n",
    "In a sample of 10 marbles, for the fraction $\\nu$ of red marbles to be $\\nu \\le 0.1$, we must have at most one red marbles.\n",
    "\n",
    "\\begin{align*}\n",
    "P(number\\;of\\;red \\le 1) &= P(red = 0) + P(red = 1) \\\\\n",
    "&= (1-\\mu)^{10} + \\mu (1-\\mu)^9\\\\\n",
    "&= (1-\\mu)^9\\\\\n",
    "&= 1.0e-9\n",
    "\\end{align*}\n",
    "\n",
    "#### Exercise 1.9\n",
    "If $μ$ = 0.9, use the Hoeffding inequality to bound the probability that a sample of 10 marbles will have $v$ <= 0.1 and compare the answer to the previous exercise.\n",
    "\n",
    "We have $\\mu=0.9$, $N=10$, and want $\\nu \\le 0.1$, i.e. $|\\mu - \\nu| = \\mu - \\nu \\ge 0.9 - 0.1 = 0.8$. Let's pick $\\epsilon = 0.7$, then according to Hoeffding Inequity, we have\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\nu \\le 0.1) &= P(\\mu - \\nu \\ge 0.8)\\\\\n",
    "&= P(|\\mu - \\nu| \\ge 0.8) \\\\\n",
    "&\\le P(|\\mu - \\nu| \\gt 0.7) \\\\\n",
    "&= P(|\\mu - \\nu| \\gt \\epsilon) \\\\\n",
    "&\\le 2e^{-2\\epsilon^2N}\\\\\n",
    "&\\approx 0.0001109032\n",
    "\\end{align*}\n",
    "\n",
    "This is an upper bound of the probability from previous problem and is much larger than the calculated probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsZZhWMBDdnS"
   },
   "source": [
    "#### Exercise 1.10\n",
    "Here is an experiment that illustrates the difference between a single bin and multiple bins. Run a computer simulation for flipping 1,000 fair coins. Flip each coin independently 10 times. Let's focus on 3 coins as follows: $c_{1}$ is the first coin flipped; $c_{rand}$ is a coin you choose at random; $c_{min}$ is the coin that had the minimum frequency of heads (pick the earlier one in case of a tie). Let $v_{1}$, $v_{rand}$ and $v_{min}$ be the fraction of heads you obtain for the respective three coins.\n",
    "\n",
    "(a) What is $μ$ for the three coins selected?\n",
    "\n",
    "(b) Repeat this entire experiment a large number of times (e.g., 100,000 runs of the entire experiment) to get several instances of $v_{1}$, $v_{rand}$ and $v_{min}$ and plot the histograms of the distributions of $v_{1}$, $v_{rand}$ and $v_{min}$. Notice that which coins end up being $c_{rand}$ and $c_{min}$ may differ from one run to another.\n",
    "\n",
    "(c) Using (b), plot estimates for P[|$v$-μ| > $\\epsilon$] as a function of $\\epsilon$, together with the Hoeffding bound $2e^{-2\\epsilon^2N}$ (on the same graph).\n",
    "\n",
    "(d) Which coins obey the Hoeffding bound, and which ones do not? Explain why.\n",
    "\n",
    "(e) Relate part (d) to the multiple bins in Figure 1.10.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1jHql9EuaaevkVGPB6ZN6__RN5YHfhkb_'>\n",
    "\n",
    "\n",
    "1. (a) The $\\mu$ for the three coins are all $0.5$ since the coins are fair."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n5xOCXx-DdnS",
    "ExecuteTime": {
     "end_time": "2025-02-18T22:37:12.206009Z",
     "start_time": "2025-02-18T22:37:12.187828Z"
    }
   },
   "source": [
    "# Exercise 1.10 (a)\n",
    "total_coins = 1000\n",
    "total_flips = 10\n",
    "run_once(total_coins, total_flips, True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of first coin: 0.5\n",
      "Frequency of a random coin: id(995)-freq(0.3)\n",
      "Frequency of the coin with minimum frequency: id(410)-freq(0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.3, 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "oKf2aA6dDdnS",
    "outputId": "925bc34f-4b6e-4e08-d413-fe1bbe776827",
    "ExecuteTime": {
     "end_time": "2025-02-18T22:37:24.551260Z",
     "start_time": "2025-02-18T22:37:13.375618Z"
    }
   },
   "source": [
    "# Exercise 1.10 (b)\n",
    "total_coins = 1000\n",
    "total_flips = 10\n",
    "total_runs = 100000\n",
    "v1s, vrands, vmins = [], [], []\n",
    "\n",
    "for run in range(total_runs):\n",
    "    v1, vrand, vmin = run_once(total_coins, total_flips)\n",
    "    v1s.append(v1)\n",
    "    vrands.append(vrand)\n",
    "    vmins.append(vmin)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\n",
    "n_bins = 10\n",
    "axs[0].hist(v1s, bins=n_bins)\n",
    "axs[1].hist(vrands, bins=n_bins)\n",
    "axs[2].hist(vmins, bins=n_bins)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.2249e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        3.7748e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]),\n",
       " array([0.  , 0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "yXIkORHfDdnS",
    "outputId": "0994c379-42eb-4495-bae4-bbd98d61ef6c",
    "ExecuteTime": {
     "end_time": "2025-02-18T22:37:29.091130Z",
     "start_time": "2025-02-18T22:37:29.012268Z"
    }
   },
   "source": [
    "# Exercise 1.10 (c)\n",
    "eps = np.arange(0.0, 0.5, 0.05)\n",
    "bounds = hoeffding_bound(eps, total_flips)\n",
    "v1s, vrands, vmins = np.array(v1s), np.array(vrands), np.array(vmins)\n",
    "v1d = np.abs(v1s - 0.5)\n",
    "vrandd = np.abs(vrands - 0.5)\n",
    "vmind = np.abs(vmins - 0.5)\n",
    "\n",
    "p1, prand, pmin = np.zeros(len(eps)), np.zeros(len(eps)), np.zeros(len(eps))\n",
    "\n",
    "for idx in range(eps.shape[0]):\n",
    "    ep = eps[idx]\n",
    "    p1[idx] = np.sum(v1d > ep) / total_runs\n",
    "    prand[idx] = np.sum(vrandd > ep) / total_runs\n",
    "    pmin[idx] = np.sum(vmind > ep) / total_runs\n",
    "\n",
    "#plt.ylim((0,0.01))\n",
    "plt.plot(eps, bounds, marker='o', markerfacecolor='blue', markersize=8, color='skyblue', label='Hoeffding Bound')\n",
    "plt.plot(eps, p1, marker='', color='r', linewidth=1, label='First Coin')\n",
    "plt.plot(eps, prand, marker='', color='g', linewidth=1, linestyle='dashed', label='Random Coin')\n",
    "plt.plot(eps, pmin, marker='', color='y', linewidth=1, linestyle='dashed', label='Coin with Minimum Freq')\n",
    "plt.legend()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20f8d1f5390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmvVCnYvDdnT"
   },
   "source": [
    "#### Exercise 1.10\n",
    "1. (d) The first and random coins follow the Hoeffding bound. The coin with minimum frequency doesn't obey Hoeffding bound. This is because that for the first two coins, the coins were chosen before the experiment. While for the last one, we have to flip all the coins first, and use the data to compute out which is the coin with minimum frequency of heads. This violates the Hoeffding inequality condition which says the hypothesis $h$ has been fixed before samples were drawn.\n",
    "\n",
    "1. (e) When we choose the coin having the minimum frequency of heads it is like choosing the bin from 1000 bins (hypothesis space). We choose the bin after we finish sampling the data. This is akin to learning algorithm for the final hypothesis. The other two coins were chosen before the sampling, which is choosing bin beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGAWINgBDdnT"
   },
   "source": [
    "#### Exercise 1.11\n",
    "We are given a data set $D$ of 25 training examples from an unknown target function $f: X -> Y$, where $X$ = $R$ and $Y$ = {-1,+1}. To learn $f$, we use a simple hypothesis set $H$ = {$h1, h2$} where $h1$ is the constant +1 function and $h2$ is the constant -1.\n",
    "\n",
    "We consider two learning algorithms, $S$ (smart) and $C$ (crazy). $S$ chooses the hypothesis that agrees the most with $D$ and $C$ chooses the other hypothesis deliberately. Let us see how these algorithms perform out of sample from the deterministic and probabilistic points of view. Assume in the probabilistic view that there is a probability distribution on $X$, and let P[$f(x) = +1$] = $p$.\n",
    "\n",
    "(a) Can $S$ produce a hypothesis that is $guaranteed$ to perform better than random on any point outside $D$?\n",
    "\n",
    "(b) Assume for the rest of the exercise that all the examples in $D$ have $y_{n}$ = +1. Is it possible that the hypothesis that $C$ produces turns out to be better than the hypothesis that $S$ produces?\n",
    "\n",
    "(c) If $p$ = 0.9, what is the probability that $S$ will produce a better hypothesis than $C$?\n",
    "\n",
    "(d) Is there any value of $p$ for which it is more likely than not that $C$ will produce a better hypothesis than $S$?\n",
    "\n",
    "1. (a) $S$ can not produce a hypothesis that is guaranteed to perform better than random on any point outside $\\mathcal{D}$.\n",
    "If $f$ has 25 $+1$ on $\\mathcal{D}$ but $-1$ on all other points in $\\mathcal{X}$, $S$ will choose the hypothesis $h_1$, which will not match $f$ outside of $\\mathcal{D}$ at all. On the other hand, a random function will have $+1$ and $-1$ 50/50, and it matches $f$ half of time, which is better than the function produced by $S$.\n",
    "\n",
    "1. (b) It is possible that $C$ produces a better hypothesis than $S$ produces. See the example above.\n",
    "\n",
    "1. (c) If every point in $\\mathcal{D}$ has 1, then $S$ will choose $h_1$ and $C$ will choose $h_2$. So outside of $\\mathcal{D}$, $h_1$ will have 90% chance to match with $f$, while $h_2$ will have only 10% chance. $S$ will always produce a better hypothesis than $C$.\n",
    "\n",
    "1. (d) From previous problem, we can see that when $p \\lt 0.5$, $C$ will produce a better hypothesis than $S$. Since $C$ always produces $h_2$, it will match $f$ better than $h_1$ if $p \\lt 0.5$.\n",
    "\n",
    "**Conclusion:** The question of whether $D$ tells us anything outside of $D$ that we didn't know before has two different answers.\n",
    "\n",
    "If we adopt a deterministic perspective, which means that $D$ tells us something certain about $f$ outside of $D$, then the answer is no.\n",
    "\n",
    "If we adopt a probabilistic pespective, which means that $D$ tells us something likely about $f$ outside of $D$, then the answer is yes. By adopting the probabilistic view, we get a positive answer to the feasibility question without paying too much of a price. The only assumption in the probabilistic framework is that the examples in $D$ are generated $independently$. Whatever distribution we use for generating the examples, we $must$ also use it when we evaluate how well $g$ approximates $f$. This makes the Hoeffding Inequality applicable.\n",
    "\n",
    "#### Exercise 1.12\n",
    "\n",
    "A friend comes to you with a learning problem. She says the target function $f$ is $completely$ unknown, but she has 4000 data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:\n",
    "\n",
    "(a) After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.\n",
    "\n",
    "(b) After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "(c) One of two things will happen:\n",
    "\n",
    "(i) You will produce a hypothesis $g$;\n",
    "\n",
    "(ii) You will declare that you failed.\n",
    "\n",
    "If you do return a hypothesis $g$, then with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
    "\n",
    "Answer:\n",
    "\n",
    "The best we can promise is (c).\n",
    "* The unknown target $f$ can be very complex that we can't learn at all.\n",
    "* If we can learn and produce a hypothesis $g$, since there are many data points (4000), the probability that $g$ matches $f$ is high according to Hoeffding inequality, and the error on $g$ might be small since we have a large data set.\n",
    "\n",
    "**Conclusion:**\n",
    "The feasibility of learning has two questions:\n",
    "\n",
    "1. Can we make $E_{out}(g)$ close enough to $E_{in}(g)$?\n",
    "2. Can we make $E_{in}(g)$ small enough?\n",
    "\n",
    "HI addresses the first question only.\n",
    "The second question is answered after we run the learning algorithm on the actual data and see how small we can get $E_{in}(g)$ to be.\n",
    "\n",
    "#### Exercise 1.13\n",
    "Consider the bin model for a hypothesis $h$ that makes an error with probability μ in approximating a deterministic target function $f$ (both $h$ and $f$ are binary functions). If we use the same $h$ to approximate a noisy version of $f$ given by\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1JYqM1j3J2icslCdWtXWOANON7AOVJpL2'>\n",
    "\n",
    "(a) What is the probability of error that $h$ makes in approximating $y$?\n",
    "\n",
    "(b) At what value of $\\lambda$ will the performance of $h$ be independent of μ? [Hint: The noisy target will look completely random.]\n",
    "\n",
    "1. (a) The probability of error that $h$ makes in approximating $y$ is\n",
    "\n",
    "\\begin{align*}\n",
    "P(h \\ne y) &= P(h \\ne y| y = f(x))P(y=f(x)) + P(h\\ne y| y \\ne f(x))P(y\\ne f(x))\\\\\n",
    "&= \\mu \\lambda + (1-\\mu)(1-\\lambda)\\\\\n",
    "&= \\mu (2 \\lambda - 1) + (1-\\lambda)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "1. (b) It can be seen from previous problem, that when $\\lambda = 0.5$, $P(h\\ne y) = 1-\\lambda = 0.5$, is independent of $\\mu$.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
