{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqSVBBZBRvFm"
      },
      "source": [
        "#### Exercise 4.1\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YeAQBDxIs7pS6EXraRH1IjiMZlnjMlGb'>\n",
        "\n",
        "* For $h(x) \\in \\mathcal{H}_2$, we have $h(x) = a_0 + a_1x + a_2x^2$, the parameters are $a_0, a_1, a_2$.\n",
        "* For $h(x) \\in \\mathcal{H}_{10}$, we have $h(x) = \\sum^{10}_{k=0} a_k x^k$.\n",
        "\n",
        "For any given $h(x) \\in \\mathcal{H}_2$, we have $h(x) = a_0 + a_1x + a_2x^2 = \\sum^{10}_{k=0} a_k x^k$ where $a_k = 0$ when $k\\gt 2$. So $h(x) \\in \\mathcal{H}_{10}$ as well. We thus conclude $\\mathcal{H}_2 \\subset \\mathcal{H}_{10}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4H-Fms6RvFo"
      },
      "source": [
        "#### Exercise 4.2\n",
        "Reading exercise from the book.\n",
        "\n",
        "#### Exercise 4.3\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1IAJNlAXzaf1oWnNtkIHyHg65GEDBYyXB'>\n",
        "\n",
        "* (a) Consider a given $\\mathcal{H}$\n",
        "  * If the best approximation from $\\mathcal{H}$ is less complex than the initial target function, then when we increase the complexity of $f$, the deterministic noise in general should increase, since it'll be harder for functions in $\\mathcal{H}$ to fit the target function. There'll be a higher tendency to overfit.\n",
        "  * If the best approximation from $\\mathcal{H}$ is more complex than the initial target function, then when we increase the complexity of $f$, the deterministic noise in general may decrease first, reducing the deterministic noise and there'll be a lower tendency to overfit. But once the complexity of $f$ exceeds the best function approximation from $\\mathcal{H}$, and if we continue increase the complexity of $f$, we will increase the deterministic noise and thus increase the tendency to overfit.\n",
        "  \n",
        "* (b) Given a fixed $f$\n",
        "  * If the best approximation from $\\mathcal{H}$ is less complex than the target function, then when we decrease the complexity of $\\mathcal{H}$, we increase the deterministic noise thus increasing the tendency of overfit.\n",
        "  * If the best approximation from $\\mathcal{H}$ is more complex than the Â§target function, then when we decrease the complexity of $\\mathcal{H}$, we will decrease the deterministic noise thus decreasing the tendency of overfit. Well, if we continue to decrease the complexity of $\\mathcal{H}$, passing the point where its complexity is equal to $f$, we start to increase the deterministic noise again and thus increasing overfit.\n",
        "  \n",
        "#### Exercise 4.4\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1V_XPDjLTwViBIFGmdpibMhUzYA0Af6d3'>\n",
        "\n",
        "Let's compute $E_{in}(w)$:\n",
        "\n",
        "\\begin{align*}\n",
        "E_{in}(w) &= \\frac{1}{N}\\sum^N_{n=1}\\left(w^Tz_n - y_n\\right)^2\\\\\n",
        "&= \\frac{1}{N} \\|Zw - y\\|^2\\\\\n",
        "&= \\frac{1}{N} (Zw - y)^T(Zw - y)\\\\\n",
        "&= \\frac{1}{N} (w^TZ^T - y^T)(Zw - y)\\\\\n",
        "&= \\frac{1}{N} w^TZ^TZw - w^TZ^Ty - y^TZw + y^Ty\\\\\n",
        "\\end{align*}\n",
        "\n",
        "On the other hand, the equation (4.3) is\n",
        "\n",
        "\\begin{align*}\n",
        "E_{in}(w) &= \\frac{(w-w_{lin})^TZ^TZ(w-w_{lin})+y^T(I-H)y}{N}\\\\\n",
        "&= \\frac{(w^T-w^T_{lin})Z^TZ(w-w_{lin})+y^Ty-y^THy}{N}\\\\\n",
        "&= \\frac{w^TZ^TZw-w^TZ^TZw_{lin}-w^T_{lin}Z^TZw+w^T_{lin}Z^TZw_{lin}+y^Ty-y^THy}{N}\\\\\n",
        "&= \\frac{w^TZ^TZw-w^TZ^TZ(Z^TZ)^{-1}Z^Ty-\\left((Z^TZ)^{-1}Z^Ty\\right)^TZ^TZw}{N}\\\\\n",
        "&+\\frac{\\left((Z^TZ)^{-1}Z^Ty\\right)^TZ^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
        "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZ(Z^TZ)^{-T}Z^TZw}{N}\\\\\n",
        "&+\\frac{y^TZ(Z^TZ)^{-T}Z^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
        "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZ(Z^TZ)^{-1}Z^TZw}{N}\\\\\n",
        "&+\\frac{y^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
        "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZw+y^Ty}{N}\\\\\n",
        "\\end{align*}\n",
        "\n",
        "This agrees with the result derived above. So we proved equation (4.3).\n",
        "\n",
        "* (a) The value of $w$ that minimizes $E_{in}$ is $w_{lin}$, since it makes the first term $0$, which is a quadratic term on $w$ and is greater or equal to $0$ all the time. The second term doesn't depend on $w$.\n",
        "\n",
        "* (b) The minimum in-sample error when $w=w_{lin}$ is simply the second term, i.e. $\\min E_{in}(w) = \\frac{y^T(I-H)y}{N}$.\n",
        "\n",
        "#### Exercise 4.5 [Tikhonov regularizer]\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1TbeTXiB_w3EAwlmvQjUiURoI10d4TMFt'>\n",
        "\n",
        "\n",
        "* (a) $\\sum^Q_{q=0}w^2_q = [w_1, w_2, \\dots, w_Q]\\begin{bmatrix}w_1\\\\ w_2\\\\ \\dots\\\\ w_Q\\end{bmatrix} = w^Tw$, so $\\Gamma = I$.\n",
        "\n",
        "* (b) $\\left(\\sum^Q_{q=0}w_q\\right)^2 = [w_1, w_2, \\dots, w_Q]\\begin{bmatrix}1\\\\1\\\\\\dots\\\\1\\end{bmatrix}[1,1,\\dots,1]\\begin{bmatrix}w_1\\\\ w_2\\\\ \\dots\\\\ w_Q\\end{bmatrix}$, so $\\Gamma = [1,1,\\dots,1]$.\n",
        "\n",
        "#### Exercise 4.6\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1nfARR8D_oEwaeG7M8CiP9Km4XBXw5ns1'>\n",
        "\n",
        "If we use the hard-order constraint, with less parameters, the perceptron's VC dimension decreases, and it's less likely to classify the same amount of points with more parameters. If we use the soft-order constraint, it won't change the signs of $(w^Tx)$ even when $w$ is small. So we'll still be able to classify the points.\n",
        "\n",
        "#### Exercise 4.7\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_hVFW9a2we-xwBAfm-jnYmMVdycMZ5bP'>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1jUx7-M1ixLEWEuWVzVClJtysyhXY50yN'>\n",
        "\n",
        "* (a) Note that the expectation w.r.t. $\\mathcal{D}_{val}$ is equivalent to $x$ because the $y$ are assumed to be generated by a true $f(x)$.\n",
        "\n",
        "\\begin{align*}\n",
        "\\sigma^2_{val} &= Var_{\\mathcal{D}_{val}}\\left[E_{val}(g^{-})\\right]\\\\\n",
        "&= Var_{\\mathcal{D}_{val}}\\left[\\frac{1}{K}\\sum_{x_n \\in \\mathcal{D}_{val}} e\\left(g^{-}\n",
        "(x_n),y_n\\right)\\right]\\\\\n",
        "&= \\frac{1}{K^2}\\left[Var_{\\mathcal{D}_{val}}\\sum_{x_n \\in \\mathcal{D}_{val}} e\\left(g^{-}\n",
        "(x_n),y_n\\right)\\right]\\\\\n",
        "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}} Var_{\\mathcal{D}_{val}} \\left[e\\left(g^{-}\n",
        "(x_n),y_n\\right)\\right]\\right]\\\\\n",
        "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}} Var_{x} \\left[e\\left(g^{-}\n",
        "(x_n),y_n\\right)\\right]\\right]\\\\\n",
        "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}}\\sigma^2(g^{-})\\right]\\\\\n",
        "&= \\frac{1}{K}\\sigma^2(g^{-})\\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "* (b) In classification problem, $e\\left(g^{-}(x), y\\right) = 1(g^{-}(x) \\ne y)$. We have\n",
        "\n",
        "\\begin{align*}\n",
        "E_x\\left[e\\left(g^{-}(x), y\\right)\\right] & = P(g^{-}(x) \\ne y)\\times 1 + P(g^{-}(x) = y)\\times 0\\\\\n",
        "& = P(g^{-}(x) \\ne y)\\\\\n",
        "\\end{align*}\n",
        "\n",
        "So the variance is:\n",
        "\n",
        "\\begin{align*}\n",
        "\\sigma^2(g^{-}) &= Var_x\\left[e\\left(g^{-}(x), y\\right)\\right] \\\\\n",
        "&= E_x\\left[\\left(e - E_x[e]\\right)^2\\right]\\\\\n",
        "&= P(g^{-}(x) \\ne y)\\left[(1-E_x[e])^2\\right] + \\left(1-P(g^{-}(x) \\ne y)\\right)\\left[(0-E_x[e])^2\\right]\\\\\n",
        "&= P(1-P)^2 + (1-P)P^2\\\\\n",
        "&= P(1-P)\\\\\n",
        "\\end{align*}\n",
        "\n",
        "* (c) In the end\n",
        "\n",
        "\\begin{align*}\n",
        "\\sigma^2_{val} &= \\frac{1}{K}\\sigma^2(g^{-})\\\\\n",
        "&= \\frac{P(1-P)}{K}\\\\\n",
        "&= \\frac{-(P-0.5)^2+0.25}{K}\\\\\n",
        "&\\le \\frac{1}{4K}\\\\\n",
        "\\end{align*}\n",
        "\n",
        "* (d) The squared error $e\\left(g^{-}(x),y\\right)$ is unbounded. The variance of it is also unbounded. So there's no uniform upper bound for $Var_{\\mathcal{D}_{val}}\\left[E_{val}(g^{-})\\right] = \\frac{1}{K}\\sigma^2(g^{-})$.\n",
        "\n",
        "* (e) For regression with squared error, if we train using fewer points (smaller $N-K$) to get $g^{-}$, then the resulting $g^{-}$ will be worse, the expectation of the squared error $E\\left[e\\left(g^{-}(x),y\\right)\\right]$ becomes larger. For continuous, non-negative random variables, higher mean often implies higher variance, so $\\sigma^2(g^{-})$ will be higher.\n",
        "\n",
        "* (f) When we increasing the size of validation set $K$, the error between $E_{val}(g^{-})$ and $E_{out}(g^{-})$ is $\\frac{\\sigma(g^{-})}{\\sqrt{K}}$. It can drop in the case of classification. But for regression, it depends on which of $\\sigma(g^{-})$ or $K$ increases faster, so the  $E_{val}(g^{-})$ as an estimate of $E_{out}$ can become worse or better.\n",
        "\n",
        "Does it mean for classification the estimate will always become better when we increase the K?\n",
        "\n",
        "But note, the $E_{out}(g^{-})$ is only for the hypothesis $g^{-}$, which can be pretty bad when $K$ is large. So for classification problem, even the error between $E_{out}(g^{-})$ and $E_{val}(g^{-})$ goes to zero, but the  $E_{out}(g^{-})$ can be quite large.\n",
        "\n",
        "\n",
        "#### Exercise 4.8\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1iQubk6ZRMm4lExErwQFy77D2P288bjLZ'>\n",
        "\n",
        "For each model $\\mathcal{H}_m$, $g^-_m$ is independently learned of from the validation set. When we take the expectation w.r.t. validation data set, the $g^-_m$ doesn't change, the expectation depends only on $x_n$, as in the derivation of equation (4.8).\n",
        "Thus $E_m$ is an unbiased estimate for the out-of-sample error $E_{out}(g^-_m)$.\n",
        "\n",
        "#### Exercise 4.9\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1O6L7EoufbT349s2APyPtMIB_wUjzcU2F'>\n",
        "\n",
        "When $K$ increases, there are less points available to train the models, so the learned models become worse, and $E_{val}(g^-_m)$ becomes larger for each model $m$. $g^-_{m^*}$ is the model with the lowest validation error among all models, so  $E_{val}(g^-_{m^*})$ will also increases with $k$. The same logic applies to $E_{out}(g^-_{m^*})$ as well since $E_{val}(g^-_{m^*})$ is an estimate of $E_{out}(g^-_{m^*})$.\n",
        "\n",
        "When $K$ increases to certain large value, there are much less points $N-K$ available to train the models, complex models converge to simple models(TODO), so the optimistic validation error $E_{val}(g^-_{m^*})$  is closer to all validation errors $E_{val}(g^-_{m})$ for all models. Since each single validation error $E_{val}(g^-_{m})$  is an unbiased estimate of the $E_{out}(g^-_{m})$, the optimistic validation error $E_{val}(g^-_{m^*})$ converges to $E_{out}(g^-_{m^*})$ as well.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}