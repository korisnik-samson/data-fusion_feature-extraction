{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yKVQtMrfArX"
   },
   "source": [
    "# Chapter 2 Training and Testing\n",
    "## Exercises\n",
    "\n",
    "#### Exercise 2.1\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1VA5JobU1X3ftJoApkNwsWqaeRKuBsyj4'>\n",
    "\n",
    "1. Positive rays: break point $k = 2$ and $m_{\\mathcal{H}}(k) = k+1 = 3 \\lt 2^k = 4$.\n",
    "1. Positive intervals: break point $k = 3$ and $m_{\\mathcal{H}}(k) = {k+1 \\choose 2} +1 = 7 \\lt 2^k = 8$.\n",
    "1. Convex sets: no break point exists. For any $k$, we can find a set of $k$ points on a circle that can be shattered.\n",
    "\n",
    "#### Exercise 2.2\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1qjY2VIrjF-S5WDmw54wslOCayzfPIffe'>\n",
    "\n",
    "1. (a)\n",
    "  * (i) $k=2$, $RHS = \\sum^{1}_{i=0}{N \\choose i} = N+1$, while $m_{\\mathcal{H}}(N) = N+1$, so $m_{\\mathcal{H}}(N) \\le \\sum^{k}_{i=0}{N \\choose i}$\n",
    "  * (ii) $k=3$, $RHS = \\sum^{2}_{i=0}{N \\choose i} = \\frac{N(N-1)}{2} + N+1 = \\frac{N(N+1)}{2} + 1$, while $m_{\\mathcal{H}}(N) = {N+1 \\choose 2}+1 = \\frac{N(N+1)}{2} + 1$, so $m_{\\mathcal{H}}(N) \\le \\sum^{k}_{i=0}{N \\choose i}$\n",
    "  * (iii) There's no such $k$ exists. Maximum $k = N+1$, since $\\sum^{N}_{i=0}{N \\choose i} = 2^N$, we still have $m_{\\mathcal{H}}(N) \\le \\sum^{k}_{i=0}{N \\choose i}$\n",
    "  \n",
    "1. (b) If $m_{\\mathcal{H}}(N) = N+2^{\\frac{N}{2}}$, then the break point $k=3$. According to bound theorem 2.4, we have for all $N$, $m_{\\mathcal{H}}(N) = N+2^{\\frac{N}{2}} \\le \\sum^{2}_{i=0}{N \\choose i} = \\frac{N(N+1)}{2} + 1$. But this won't hold for all $N$ since left hand side is exponentially increasing while the RHS is polynomical increasing. For example, when $N=20$, the inequality breaks. So such hypothesis set doesn't exist.\n",
    "\n",
    "#### Exercise 2.3\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1ueCjmZPNhXJz2poG7gxaMS2tj7yo0csR'>\n",
    "\n",
    "1. (i) $d_{VC} = 1$\n",
    "1. (ii) $d_{VC} = 2$\n",
    "1. (iii) $d_{VC} = \\infty$\n",
    "\n",
    "\n",
    "#### Exercise 2.5\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1r7C3bBob3xwIrOTp7LJ5CWDWYt7IKCg6'>\n",
    "\n",
    "Through equation (2.12), we find that $\\delta = 709.527509678$, so the probability is just greater or equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bbWsEkO4fArc",
    "outputId": "51fa4ac7-1d35-4827-ed9d-ec1520c64cd5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-04-01T12:17:52.129854Z",
     "start_time": "2025-04-01T12:17:51.990300Z"
    }
   },
   "source": [
    "# Exercise 2.5\n",
    "import numpy as np\n",
    "N = 100\n",
    "d = 0.1\n",
    "mh = 2*N + 1\n",
    "delta = 4*mh / np.exp(N * d**2 /8)\n",
    "delta, np.exp(N * d**2 /8)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709.5275096780147, 1.1331484530668263)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_zFdfNefArd"
   },
   "source": [
    "#### Exercise 2.6\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1oZdi7QYdgXzxIHPd-WZUb8FkYLfrDe7e'>\n",
    "\n",
    "* (a) Apply the error bar in $(2.1)$, i.e. $E_{out}(g) \\le E_{in}(g) + \\sqrt{\\frac{1}{2N}\\ln{\\frac{2M}{\\delta}}}$.\n",
    "\n",
    "The following calculation shows that error on $E_{in}(g) = 0.1151$ and error on $E_{test}(g) = 0.096$. So the error bar on in-sample error is higher than the error bar from test error.\n",
    "\n",
    "* (b) If we reserve more examples for testing, then we'll have less samples for training. We may end up with a hypothesis that is not as good as we could have arrived if using more training samples. So $E_{test}(g)$ might be way too off even the error bar on it is small.\n",
    "\n",
    "**Test set and test error:**\n",
    "\n",
    "Estimating $E_{out}$ using a test set, a data set that was not involved in the training process.\n",
    "\n",
    "The final hypothesis $g$ is evaluated on the test set and the result $E_{test}$ is taken as an estimate of $E_{out}$.\n",
    "\n",
    "The bigger the test set, the more accurate $E_{test}$ will be as an estimate of $E_{out}$.\n",
    "\n",
    "On the other hand, the test set doesn't affect the learning process, just indicates how well we learned. The training set is essential to find a good hypothesis. The larger chunk of test data, the smaller size od the training sample, meaning we might not get a good hypothesis from the training part even if we can reliably evaluate it in the testing part. So we need to set aside test examples carefully!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tEd81vB2fAre",
    "outputId": "0f19c646-c7d0-47bd-93f0-90ef3352913a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-04-01T12:17:52.154197Z",
     "start_time": "2025-04-01T12:17:52.142227Z"
    }
   },
   "source": [
    "# Exercise 2.6\n",
    "import numpy as np\n",
    "epsilon = 0.05\n",
    "\n",
    "N = 200\n",
    "# test bound\n",
    "print('test bound: ', np.sqrt(np.log(2/epsilon)/2/N))\n",
    "\n",
    "# train bound\n",
    "M = 1000\n",
    "N = 400\n",
    "print('train bound: ', np.sqrt(np.log(2*M/epsilon)/2/N))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test bound:  0.09603227913199208\n",
      "train bound:  0.11509037065006825\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pslYDGscfAre"
   },
   "source": [
    "#### Exercise 2.7\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1sPDlFlbVb_ac8jtubHCAT2fBAIentN7N'>\n",
    "\n",
    "1. (a)\n",
    "\n",
    "\\begin{align*}\n",
    "P[h(x)\\ne f(x)] &= P[h(x)\\ne f(x)]\\cdot 1 + P[h(x) = f(x)]\\cdot0 \\\\\n",
    "&= P[h(x)\\ne f(x)] (h(x)-f(x))^2 + P[h(x) = f(x)] (h(x)-f(x))^2 \\\\\n",
    "&= E[(h(x)-f(x))^2]\n",
    "\\end{align*}\n",
    "\n",
    "1. (b)\n",
    "\n",
    "\\begin{align*}\n",
    "P[h(x)\\ne f(x)] &= \\frac{1}{4}P[h(x)\\ne f(x)]\\cdot 4 + \\frac{1}{4}P[h(x) = f(x)]\\cdot0 \\\\\n",
    "&= \\frac{1}{4}P[h(x)\\ne f(x)] (h(x)-f(x))^2 + \\frac{1}{4}P[h(x) = f(x)] (h(x)-f(x))^2 \\\\\n",
    "&= \\frac{1}{4}E[(h(x)-f(x))^2]\n",
    "\\end{align*}\n",
    "\n",
    "**Other target types:**\n",
    "\n",
    "To deal with real-valued funtions, we need to adapt the definitions of $E_{in}$ and $E_{out}$ that have so far been used for binary functions (as binary error).\n",
    "\n",
    "When $f$ and $h$ are real-valued, an appropriate error measure would gauge how far $f(x)$ and $h(x)$ are from each other, rather than whether their values are the same. The squared error is commonly used.\n",
    "\n",
    "The exercise shows that the squared error can also by used as the error measure for binary functions!\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1_d7PhCEa1BbJ24J83miaDe-i5O5MwmGk'>\n",
    "\n",
    "#### Exercise 2.8\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1xO3RPDDzL5o7gSQpEP_7Q8mkwaWpStj5'>\n",
    "\n",
    "1. (a) If $\\mathcal{H}$ is closed under linear combination, for any $x$, $\\bar{g}(x)$ is weighted (by probability of data) average of hypotheses in $\\mathcal{H}$, so $\\bar{g}(x) \\in \\mathcal{H}$.\n",
    "\n",
    "1. (b) If $\\mathcal{H}$ is a set of functions defined on intervals, e.g. $f(x) = c$ when $x \\in [a,b]$, otherwise $f(x) = 0$. Then $\\bar{g}(x)$ probably won't have constant value in an interval and not in the original hypothesis set.\n",
    "\n",
    "1. (c) For binary classification, each $g(x)$ will have value $+1$ or $-1$, when weighted by probabilities, the average is not binary any more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
